"""
Memory Forest ì»´íŒ©íŠ¸ DAG - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í¬í•¨
ë‘ ê°€ì§€ í•µì‹¬ ê¸°ëŠ¥ì— ì§‘ì¤‘:
1. ì‹¤íŒ¨/ëŒ€ê¸° ìƒíƒœ ê²Œì„ë“¤ì˜ AI ì¬ë¶„ì„
2. ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´ë“¤ì˜ í•™ìŠµ (Optuna ìµœì í™” í¬í•¨)
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime, timedelta
import pendulum
import logging
import csv
import time
import os
import shutil
import pandas as pd
import optuna
from gensim.models import Word2Vec
from numpy import dot
from numpy.linalg import norm
import numpy as np
import requests
import json
import re
import urllib.request
import urllib.parse
from typing import Dict, List, Tuple, Optional

# ë¡œì»¬ ëª¨ë“ˆ import
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import DEFAULT_ARGS, AI_STATUS_CODES, SCHEDULES, BATCH_CONFIG, MODEL_CONFIG, NAVER_API_CONFIG
from utils import db_manager, ai_client, text_processor

logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
dag_default_args = {
    **DEFAULT_ARGS,
    'start_date': pendulum.datetime(2024, 1, 1, tz=pendulum.timezone("Asia/Seoul")),
}

class OptimizedWord2VecTrainer:
    """Word2Vec ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self):
        # utils/__init__.pyì—ì„œ ì´ë¯¸ ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        self.text_processor = text_processor
        self.model_path = MODEL_CONFIG['model_path']
        self.backup_path = MODEL_CONFIG['backup_path']
        self.train_stats_path = "/opt/airflow/data/word2vec_train_stats.csv"
        
        # í‰ê°€ìš© í‘œì¤€ ë‹¨ì–´ ì„¸íŠ¸ (ì¹˜ë©” í˜¹ì€ ë…¸ì¸ë“¤ì´ ìì£¼ ì“¸ë²•í•œ ë‹¨ì–´ë“¤)
        self.evaluation_words = [
            "ë¶€ëª¨", "ìœ ëŸ½", "ê¸°ì¨", "ê³ ì–‘ì´", "ê¹€ì¹˜ì°Œê°œ", "ì¶œê·¼", "ì¶•êµ¬", "ì¹«ì†”",
            "ëƒ‰ì¥ê³ ", "ì„ ì¸ì¥", "í˜¸ë‘ì´", "ë¶ˆêµ", "ë³‘ì›", "ì²­ë°”ì§€", "ì¡¸ì—…ì‹", "ë…ì„œ",
            "ë´„ë¹„", "êµê³¼ì„œ", "ê°„í˜¸ì‚¬", "ë¶€ì‚°", "ì§€í•˜ì² ", "ë°•ë¬¼ê´€", "ì—°ê·¹", "ìƒì¼", "ìœ ì¹˜ì›"
        ]
    
    def preprocess_sentences(self, sentences: List[str], progress_callback=None) -> List[List[str]]:
        """ë¬¸ì¥ë“¤ì„ ì „ì²˜ë¦¬í•˜ì—¬ í† í°í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ - ì™„ì „ ë¹ ë¥¸ ë°©ì‹"""
        logger.info(f"ğŸ“ ë¹ ë¥¸ ë¬¸ì¥ ì „ì²˜ë¦¬ ì‹œì‘: {len(sentences)}ê°œ ë¬¸ì¥")
        
        tokenized_sentences = []
        processed_count = 0
        batch_size = 30  # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸°
        
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i+batch_size]
            
            for j, sentence in enumerate(batch):
                if not sentence or len(sentence.strip()) < 5:
                    continue
                
                # ë¹ ë¥¸ í† í°í™” (KoNLPy ì—†ì´)
                tokens = self.fast_tokenize(sentence)
                if len(tokens) >= 2:  # ìµœì†Œ 2ê°œ í† í° ì´ìƒ
                    tokenized_sentences.append(tokens)
                    processed_count += 1
                
                # ì§„í–‰ë¥  ë³´ê³  (ë°°ì¹˜ë§ˆë‹¤)
                if progress_callback and (i + j + 1) % batch_size == 0:
                    progress = (i + j + 1) / len(sentences) * 100
                    progress_callback(f"ë¹ ë¥¸ ì „ì²˜ë¦¬ ì§„í–‰ë¥ : {progress:.1f}% ({processed_count}ê°œ ìœ íš¨ ë¬¸ì¥)")
        
        logger.info(f"âœ… ë¹ ë¥¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(sentences)}ê°œ â†’ {len(tokenized_sentences)}ê°œ ìœ íš¨ ë¬¸ì¥")
        return tokenized_sentences
    
    def fast_tokenize(self, text: str) -> List[str]:
        """ë¹ ë¥¸ í† í°í™” (TextProcessor ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ìš©)"""
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<.*?>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_text = re.sub(r'[^\w\sê°€-í£]', ' ', clean_text)
        # ì—°ì† ê³µë°± ì •ë¦¬
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # í•œê¸€ ë‹¨ì–´ë§Œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        tokens = re.findall(r'[ê°€-í£]{2,8}', clean_text)
        
        # ì¤‘ë³µ ì œê±°
        unique_tokens = []
        seen = set()
        for token in tokens:
            if token not in seen:
                unique_tokens.append(token)
                seen.add(token)
        
        return unique_tokens
    
    def calculate_similarity_score(self, model: Word2Vec, words: List[str]) -> float:
        """ëª¨ë¸ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            vectors = [model.wv[word] for word in words if word in model.wv]
            if len(vectors) < 2:
                return 0.0
                
            similarities = []
            for i in range(len(vectors)):
                for j in range(i + 1, len(vectors)):
                    sim = dot(vectors[i], vectors[j]) / (norm(vectors[i]) * norm(vectors[j]))
                    similarities.append(sim)
            
            return round(np.mean(similarities), 4) if similarities else 0.0
        except Exception as e:
            logger.warning(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def get_baseline_score(self) -> float:
        """ê¸°ì¡´ ëª¨ë¸ì˜ ë² ì´ìŠ¤ë¼ì¸ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°"""
        if os.path.exists(self.model_path):
            try:
                baseline_model = Word2Vec.load(self.model_path)
                baseline_score = self.calculate_similarity_score(baseline_model, self.evaluation_words)
                logger.info(f"ğŸ“Š ê¸°ì¤€ ëª¨ë¸ í‰ê·  ìœ ì‚¬ë„: {baseline_score}")
                return baseline_score
            except Exception as e:
                logger.warning(f"ê¸°ì¤€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ğŸ†• ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ - ì´ˆê¸° í•™ìŠµ")
        return 0.0
    
    def optimize_hyperparameters(self, tokenized_sentences: List[List[str]], 
                                n_trials: int = 15, progress_callback=None) -> Dict:
        """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        logger.info(f"ğŸ”¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ ({n_trials}íšŒ ì‹œë„)")
        
        trial_count = 0
        best_score = 0.0
        
        def objective(trial):
            nonlocal trial_count, best_score
            trial_count += 1
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜
            vector_size = trial.suggest_categorical("vector_size", [50, 100, 150, 200])
            window = trial.suggest_int("window", 3, 10)
            min_count = trial.suggest_int("min_count", 2, 8)
            epochs = trial.suggest_int("epochs", 5, 20)
            sg = trial.suggest_categorical("sg", [0, 1])  # CBOW vs Skip-gram
            alpha = trial.suggest_float("alpha", 0.01, 0.05)
            
            try:
                # ëª¨ë¸ í•™ìŠµ
                model = Word2Vec(
                    sentences=tokenized_sentences,
                    vector_size=vector_size,
                    window=window,
                    min_count=min_count,
                    workers=4,
                    sg=sg,
                    epochs=epochs,
                    alpha=alpha
                )
                
                # ì„±ëŠ¥ í‰ê°€
                score = self.calculate_similarity_score(model, self.evaluation_words)
                
                if score > best_score:
                    best_score = score
                
                # ì§„í–‰ë¥  ë³´ê³ 
                if progress_callback:
                    progress = trial_count / n_trials * 100
                    progress_callback(f"ìµœì í™” ì§„í–‰ë¥ : {progress:.1f}% (ì‹œë„ {trial_count}/{n_trials}, í˜„ì¬ ì ìˆ˜: {score:.4f}, ìµœê³  ì ìˆ˜: {best_score:.4f})")
                
                logger.info(f"ğŸ§ª ì‹œë„ {trial_count}/{n_trials}: ì ìˆ˜ {score:.4f} (íŒŒë¼ë¯¸í„°: vec={vector_size}, win={window}, cnt={min_count}, ep={epochs}, sg={sg})")
                
                return score
                
            except Exception as e:
                logger.warning(f"âŒ ì‹œë„ {trial_count} ì‹¤íŒ¨: {e}")
                return 0.0
        
        # Optuna ìŠ¤í„°ë”” ì‹¤í–‰
        study = optuna.create_study(direction="maximize")
        study.optimize(objective, n_trials=n_trials)
        
        best_params = study.best_params
        best_value = study.best_value
        
        logger.info(f"ğŸ† ìµœì í™” ì™„ë£Œ!")
        logger.info(f"   ìµœê³  ì ìˆ˜: {best_value:.4f}")
        logger.info(f"   ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return best_params, best_value
    
    def train_optimized_model(self, tokenized_sentences: List[List[str]], 
                            best_params: Dict, baseline_score: float,
                            progress_callback=None) -> Tuple[bool, Dict]:
        """ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµ"""
        logger.info(f"ğŸ¯ ê¸°ì¡´ ëª¨ë¸ì— ìƒˆ ë°ì´í„° ì¶”ê°€ í•™ìŠµ ì‹œì‘")
        
        start_time = time.time()
        
        try:
            if progress_callback:
                progress_callback("ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ ì „ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
            
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            if os.path.exists(self.model_path):
                logger.info(f"ğŸ“š ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {self.model_path}")
                model = Word2Vec.load(self.model_path)
                
                # í•™ìŠµ ì „ ì„±ëŠ¥ ì¸¡ì • (í˜„ì¬ ì‹œì ì˜ ì‹¤ì œ ê¸°ì¤€ì )
                pre_training_score = self.calculate_similarity_score(model, self.evaluation_words)
                logger.info(f"ğŸ“Š í•™ìŠµ ì „ í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥: {pre_training_score:.4f}")
                
                # ìƒˆ ë°ì´í„°ë¡œ vocabulary ì—…ë°ì´íŠ¸
                logger.info(f"ğŸ“– ìƒˆ ë‹¨ì–´ë“¤ì„ ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€...")
                original_vocab_size = len(model.wv)
                model.build_vocab(tokenized_sentences, update=True)
                new_vocab_size = len(model.wv)
                added_words = new_vocab_size - original_vocab_size
                logger.info(f"ğŸ“ˆ ì–´íœ˜ í¬ê¸°: {original_vocab_size} â†’ {new_vocab_size} ({added_words}ê°œ ë‹¨ì–´ ì¶”ê°€)")
                
                # ì¶”ê°€ í•™ìŠµ (ê¸°ì¡´ ì§€ì‹ ìœ ì§€í•˜ë©´ì„œ ìƒˆ ë°ì´í„° í•™ìŠµ)
                logger.info(f"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì— ìƒˆ ë°ì´í„° ì¶”ê°€ í•™ìŠµ...")
                model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=best_params["epochs"])
                
            else:
                # ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                logger.info(f"ğŸ†• ìƒˆ ëª¨ë¸ ìƒì„±...")
                pre_training_score = 0.0  # ê¸°ì¡´ ëª¨ë¸ì´ ì—†ìœ¼ë¯€ë¡œ 0
                model = Word2Vec(
                    sentences=tokenized_sentences,
                    vector_size=best_params["vector_size"],
                    window=best_params["window"],
                    min_count=best_params["min_count"],
                    workers=4,
                    sg=best_params.get("sg", 1),
                    epochs=best_params["epochs"],
                    alpha=best_params.get("alpha", 0.025)
                )
            
            # í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€
            post_training_score = self.calculate_similarity_score(model, self.evaluation_words)
            train_time = time.time() - start_time
            
            logger.info(f"ğŸ“Š í•™ìŠµ í›„ ëª¨ë¸ ì„±ëŠ¥: {post_training_score:.4f}")
            logger.info(f"ğŸ“Š í•™ìŠµ ì „ ê¸°ì¤€ ì„±ëŠ¥: {pre_training_score:.4f}")
            logger.info(f"ğŸ“Š ì„±ëŠ¥ ë³€í™”: {post_training_score - pre_training_score:+.4f}")
            logger.info(f"â±ï¸ í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
            
            # ì„±ëŠ¥ ë³€í™” í—ˆìš© ì„ê³„ê°’ (í•™ìŠµ ì „ ì„±ëŠ¥ ê¸°ì¤€)
            performance_threshold_ratio = 0.05  # 5% ì„±ëŠ¥ ì €í•˜ê¹Œì§€ í—ˆìš©
            min_acceptable_score = pre_training_score * (1 - performance_threshold_ratio)
            
            logger.info(f"ğŸ¯ ìµœì†Œ í—ˆìš© ì„±ëŠ¥: {min_acceptable_score:.4f} (ê¸°ì¤€ì˜ {(1-performance_threshold_ratio)*100}%)")
            
            if post_training_score >= min_acceptable_score:
                # ì„±ëŠ¥ì´ í—ˆìš© ë²”ìœ„ ë‚´ì—ì„œ ìœ ì§€ë¨
                performance_change = ((post_training_score - pre_training_score) / pre_training_score * 100) if pre_training_score > 0 else 0
                logger.info(f"âœ… ì„±ëŠ¥ í—ˆìš© ë²”ìœ„ ë‚´ ({performance_change:+.2f}%) - ëª¨ë¸ ì €ì¥")
                
                # ê¸°ì¡´ ëª¨ë¸ ë°±ì—…
                if os.path.exists(self.backup_path):
                    if os.path.isdir(self.backup_path):
                        shutil.rmtree(self.backup_path)
                    else:
                        os.remove(self.backup_path)
                shutil.copy2(self.model_path, self.backup_path)
                logger.info(f"ğŸ’¾ ê¸°ì¡´ ëª¨ë¸ ë°±ì—… ì™„ë£Œ")
                
                # ìƒˆ ëª¨ë¸ ì €ì¥
                model.save(self.model_path)
                logger.info(f"ğŸ’¾ ì¶”ê°€ í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_path}")
                
                # í•™ìŠµ í†µê³„ ì €ì¥
                self.save_training_stats(model, post_training_score, train_time, best_params, False)
                
                return True, {
                    'success': True,
                    'pre_training_score': pre_training_score,
                    'post_training_score': post_training_score,
                    'performance_change': post_training_score - pre_training_score,
                    'train_time': train_time,
                    'vocab_size': len(model.wv),
                    'restored': False,
                    'training_mode': 'incremental'
                }
            else:
                # ì„±ëŠ¥ì´ í—ˆìš© ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨
                performance_drop = ((pre_training_score - post_training_score) / pre_training_score * 100) if pre_training_score > 0 else 0
                logger.warning(f"âš ï¸ ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’ ì´ˆê³¼ ({performance_drop:.2f}% ì €í•˜) - ê¸°ì¡´ ëª¨ë¸ ìœ ì§€")
                
                # ê¸°ì¡´ ëª¨ë¸ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì´ë¯¸ ë¡œë“œëœ ìƒíƒœì´ë¯€ë¡œ ë‹¤ì‹œ ë¡œë“œ)
                if os.path.exists(self.model_path):
                    baseline_model = Word2Vec.load(self.model_path)
                else:
                    baseline_model = model  # ìƒˆ ëª¨ë¸ì´ì—ˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                
                self.save_training_stats(baseline_model, pre_training_score, train_time, best_params, True)
                
                return False, {
                    'success': False,
                    'pre_training_score': pre_training_score,
                    'post_training_score': post_training_score,
                    'performance_change': post_training_score - pre_training_score,
                    'train_time': train_time,
                    'vocab_size': len(baseline_model.wv),
                    'restored': True,
                    'reason': f'ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’ ì´ˆê³¼ ({performance_drop:.2f}% ì €í•˜)',
                    'training_mode': 'incremental'
                }
                    
        except Exception as e:
            logger.error(f"âŒ ì¶”ê°€ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False, {
                'success': False,
                'error': str(e)
            }
    
    def save_training_stats(self, model: Word2Vec, score: float, train_time: float, 
                          params: Dict, restored: bool):
        """í•™ìŠµ í†µê³„ë¥¼ CSV íŒŒì¼ì— ì €ì¥"""
        try:
            train_date = datetime.now().strftime("%Y-%m-%d")
            train_time_str = datetime.now().strftime("%H:%M:%S")
            
            stats = {
                'date': train_date,
                'time': train_time_str,
                'vocab_size': len(model.wv),
                'vector_size': params.get('vector_size', model.vector_size),
                'window': params.get('window', model.window),
                'min_count': params.get('min_count', model.min_count),
                'epochs': params.get('epochs', model.epochs),
                'sg': params.get('sg', model.sg),
                'alpha': params.get('alpha', model.alpha),
                'avg_similarity': score,
                'train_time_sec': round(train_time, 2),
                'corpus_count': model.corpus_count,
                'corpus_total_words': model.corpus_total_words,
                'restored': 'yes' if restored else 'no'
            }
            
            # CSV íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            file_exists = os.path.exists(self.train_stats_path)
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(self.train_stats_path), exist_ok=True)
            
            with open(self.train_stats_path, mode='a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                
                # í—¤ë” ì“°ê¸° (íŒŒì¼ì´ ìƒˆë¡œ ìƒì„±ëœ ê²½ìš°)
                if not file_exists:
                    writer.writerow(stats.keys())
                
                # ë°ì´í„° ì“°ê¸°
                writer.writerow(stats.values())
            
            logger.info(f"ğŸ“Š í•™ìŠµ í†µê³„ ì €ì¥ ì™„ë£Œ: {self.train_stats_path}")
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")

def train_missing_words(**context):
    """ìˆ˜ì§‘ëœ ë‹¨ì–´ë“¤ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ"""
    logger.info("=== ğŸš€ ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
    
    try:
        # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë‹¨ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        missing_words = context['task_instance'].xcom_pull(
            task_ids='collect_missing_words',
            key='missing_words'
        )
        
        if not missing_words:
            logger.info("âŒ í•™ìŠµí•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {
                'training_executed': False,
                'words_trained': 0,
                'model_reloaded': False
            }
        
        logger.info(f"ğŸ¯ ëŒ€ìƒ ë‹¨ì–´: {missing_words}")
        logger.info(f"ğŸ“Š ì´ {len(missing_words)}ê°œ ë‹¨ì–´ í•™ìŠµ ì˜ˆì •")
        
        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
        def progress_callback(message):
            logger.info(f"ğŸ“ˆ {message}")
        
        # 1ë‹¨ê³„: ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ê´€ë ¨ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        logger.info(f"\n" + "="*60)
        logger.info(f"ğŸ“ 1ë‹¨ê³„: ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"="*60)
        
        all_collected_texts = []
        
        # ê° ë‹¨ì–´ë³„ë¡œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ìµœëŒ€ 5ê°œ ë‹¨ì–´)
        max_words_to_process = min(5, len(missing_words))
        for i, word in enumerate(missing_words[:max_words_to_process]):
            progress_callback(f"í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì§„í–‰ë¥ : {(i+1)/max_words_to_process*100:.1f}% (ë‹¨ì–´: '{word}')")
            
            try:
                word_texts = collect_blog_texts_for_word(word, max_texts=150)
                if word_texts:
                    all_collected_texts.extend(word_texts)
                    logger.info(f"âœ… '{word}' ì™„ë£Œ: {len(word_texts)}ê°œ ë¬¸ì¥ ìˆ˜ì§‘")
                else:
                    logger.warning(f"âš ï¸ '{word}' ìˆ˜ì§‘ ê²°ê³¼ ì—†ìŒ")
                    
            except Exception as word_error:
                logger.error(f"âŒ '{word}' í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {word_error}")
                continue
        
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ë¬¸ì¥: {len(all_collected_texts)}ê°œ")
        
        # ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ê±´ë„ˆë›°ê¸°
        if len(all_collected_texts) < 50:
            logger.warning(f"âš ï¸ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(all_collected_texts)}ê°œ < 50ê°œ)")
            logger.info(f"ğŸ”„ AI ì„œë¹„ìŠ¤ ëª¨ë¸ ë¦¬ë¡œë“œë§Œ ìˆ˜í–‰")
            
            reload_success = ai_client.reload_model()
            return {
                'training_executed': False,
                'words_trained': 0,
                'model_reloaded': reload_success,
                'reason': 'insufficient_text'
            }
        
        # 2ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ
        logger.info(f"\nğŸ§  2ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª¨ë¸ í•™ìŠµ")
        logger.info(f"=" * 50)
        
        trainer = OptimizedWord2VecTrainer()
        
        # ë¬¸ì¥ ì „ì²˜ë¦¬
        progress_callback("ë¬¸ì¥ ì „ì²˜ë¦¬ ì‹œì‘...")
        tokenized_sentences = trainer.preprocess_sentences(all_collected_texts, progress_callback)
        
        if len(tokenized_sentences) < 30:
            logger.warning(f"âš ï¸ ìœ íš¨í•œ ë¬¸ì¥ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(tokenized_sentences)}ê°œ < 30ê°œ)")
            reload_success = ai_client.reload_model()
            return {
                'training_executed': False,
                'words_trained': 0,
                'model_reloaded': reload_success,
                'reason': 'insufficient_valid_sentences'
            }
        
        # ê¸°ì¤€ ì„±ëŠ¥ í™•ì¸
        baseline_score = trainer.get_baseline_score()
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        progress_callback("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        best_params, best_score = trainer.optimize_hyperparameters(
            tokenized_sentences, 
            n_trials=12,  # ì ë‹¹í•œ ì‹œë„ íšŸìˆ˜
            progress_callback=progress_callback
        )
        
        # ìµœì¢… ëª¨ë¸ í•™ìŠµ
        progress_callback("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        training_success, training_result = trainer.train_optimized_model(
            tokenized_sentences,
            best_params,
            baseline_score,
            progress_callback
        )
        
        # 3ë‹¨ê³„: ëª¨ë¸ ë¦¬ë¡œë“œ
        logger.info(f"\nğŸ”„ 3ë‹¨ê³„: ëª¨ë¸ ë¦¬ë¡œë“œ")
        logger.info(f"=" * 50)
        
        if training_success:
            logger.info(f"â³ AI ì„œë¹„ìŠ¤ ëª¨ë¸ ë¦¬ë¡œë“œ ì¤‘...")
            reload_success = ai_client.reload_model()
            
            if reload_success:
                logger.info(f"âœ… ëª¨ë¸ ë¦¬ë¡œë“œ ì„±ê³µ!")
                
                # 4ë‹¨ê³„: ê²Œì„ ìƒíƒœ ì—…ë°ì´íŠ¸
                logger.info(f"\nğŸ“Š 4ë‹¨ê³„: ê²Œì„ ìƒíƒœ ì—…ë°ì´íŠ¸")
                logger.info(f"=" * 50)
                
                updated_count = update_trained_word_games(missing_words)
                
                # ìµœì¢… ê²°ê³¼
                logger.info(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
                logger.info(f"=" * 50)
                logger.info(f"âœ… í•™ìŠµëœ ë‹¨ì–´: {len(missing_words)}ê°œ")
                logger.info(f"ğŸ“š ì‚¬ìš©ëœ ë¬¸ì¥: {len(tokenized_sentences)}ê°œ")
                logger.info(f"ğŸ® ì—…ë°ì´íŠ¸ëœ ê²Œì„: {updated_count}ê°œ")
                logger.info(f"ğŸ“ˆ ì„±ëŠ¥ ê°œì„ : {training_result.get('improvement', 0):.4f}")
                logger.info(f"ğŸ† ìµœì¢… ì ìˆ˜: {training_result.get('new_score', 0):.4f}")
                
                return {
                    'training_executed': True,
                    'words_trained': len(missing_words),
                    'model_reloaded': True,
                    'games_updated': updated_count,
                    'texts_collected': len(all_collected_texts),
                    'valid_sentences': len(tokenized_sentences),
                    'trained_words': missing_words,
                    'performance_score': training_result.get('new_score', 0),
                    'improvement': training_result.get('improvement', 0),
                    'best_params': best_params
                }
            else:
                logger.error("âŒ ëª¨ë¸ ë¦¬ë¡œë“œ ì‹¤íŒ¨")
                return {
                    'training_executed': True,
                    'words_trained': len(missing_words),
                    'model_reloaded': False
                }
        else:
            logger.warning(f"âš ï¸ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ ë˜ëŠ” ì„±ëŠ¥ ì €í•˜ë¡œ ì¸í•œ ë³µì›")
            
            # ë³µì›ëœ ê²½ìš°ì—ë„ ë¦¬ë¡œë“œ ì‹œë„
            reload_success = ai_client.reload_model()
            
            return {
                'training_executed': False,
                'words_trained': 0,
                'model_reloaded': reload_success,
                'reason': training_result.get('reason', 'training_failed'),
                'performance_degradation': True
            }
            
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ í•™ìŠµ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return {
            'training_executed': False,
            'words_trained': 0,
            'model_reloaded': False,
            'error': str(e)
        }

def collect_blog_texts_for_word(word: str, max_texts: int = 100) -> List[str]:
    """íŠ¹ì • ë‹¨ì–´ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    
    logger.info(f"ğŸŒ ë„¤ì´ë²„ API ì„¤ì • í™•ì¸...")
    logger.info(f"  Client ID: {NAVER_API_CONFIG['client_id'][:10] if NAVER_API_CONFIG['client_id'] else 'None'}...")
    logger.info(f"  Client Secret: {'ì„¤ì •ë¨' if NAVER_API_CONFIG['client_secret'] else 'ì—†ìŒ'}")
    
    if not NAVER_API_CONFIG['client_id'] or not NAVER_API_CONFIG['client_secret']:
        logger.error(f"âŒ ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return []
    
    def get_request_url(url):
        req = urllib.request.Request(url)
        req.add_header("X-Naver-Client-Id", NAVER_API_CONFIG['client_id'])
        req.add_header("X-Naver-Client-Secret", NAVER_API_CONFIG['client_secret'])
        try:
            response = urllib.request.urlopen(req)
            if response.getcode() == 200:
                return response.read().decode('utf-8')
            else:
                logger.error(f"âŒ API ì‘ë‹µ ì‹¤íŒ¨ (ìƒíƒœ: {response.getcode()})")
                return None
        except Exception as e:
            logger.error(f"âŒ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None
    
    def get_naver_search(query, start=1, display=50):
        base = "https://openapi.naver.com/v1/search"
        node = "/blog.json"
        parameters = f"?query={urllib.parse.quote(query)}&start={start}&display={display}"
        url = base + node + parameters
        
        response = get_request_url(url)
        if response:
            try:
                return json.loads(response)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return None
        return None
    
    def split_into_sentences(text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ - ë¹ ë¥¸ ë°©ì‹"""
        # ë¹ ë¥¸ ë¬¸ì¥ ë¶„ë¦¬ ê·œì¹™
        sentences = []
        
        # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        raw_sentences = re.split(r'[.!?ã€‚]', text)
        
        for sentence in raw_sentences:
            sentence = sentence.strip()
            
            # ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ ì œì™¸)
            if 8 <= len(sentence) <= 80:  # ê¸¸ì´ ê¸°ì¤€ ì™„í™”
                # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ ìˆ˜ì§‘
                if word in sentence:
                    sentences.append(sentence)
        
        return sentences
    
    logger.info(f"ğŸ“ '{word}' ê´€ë ¨ ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘...")
    logger.info(f"ğŸ¯ ëª©í‘œ: ìµœëŒ€ {max_texts}ê°œ ë¬¸ì¥ ìˆ˜ì§‘")
    
    collected_sentences = []
    
    try:
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ (ì—¬ëŸ¬ í˜ì´ì§€)
        max_blogs_per_page = 50
        max_pages = 2  # ìµœëŒ€ 2í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘
        
        for page in range(max_pages):
            start_idx = page * max_blogs_per_page + 1
            logger.info(f"ğŸ” í˜ì´ì§€ {page + 1} ê²€ìƒ‰ ì¤‘... (ì‹œì‘ ì¸ë±ìŠ¤: {start_idx})")
            
            search_result = get_naver_search(word, start=start_idx, display=max_blogs_per_page)
            
            if not search_result or 'items' not in search_result:
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page + 1} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue
            
            items = search_result['items']
            logger.info(f"ğŸ“Š í˜ì´ì§€ {page + 1}: {len(items)}ê°œ ë¸”ë¡œê·¸ ê¸€ ë°œê²¬")
            
            for i, item in enumerate(items):
                # descriptionì—ì„œ HTML íƒœê·¸ ì œê±°
                description = item.get('description', '')
                clean_text = re.sub(r'<[^>]+>', '', description)
                clean_text = re.sub(r'&[a-zA-Z0-9#]+;', '', clean_text)
                
                if len(clean_text) < 20:
                    continue
                
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                sentences = split_into_sentences(clean_text)
                
                for sentence in sentences:
                    if len(collected_sentences) >= max_texts:
                        logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {max_texts}ê°œ ë¬¸ì¥ ìˆ˜ì§‘ ì™„ë£Œ")
                        return collected_sentences
                    
                    collected_sentences.append(sentence)
                    
                    # ì§„í–‰ë¥  ë¡œê·¸ (10ê°œë§ˆë‹¤)
                    if len(collected_sentences) % 10 == 0:
                        logger.info(f"ğŸ“ˆ ìˆ˜ì§‘ ì§„í–‰ë¥ : {len(collected_sentences)}/{max_texts}ê°œ ë¬¸ì¥")
                
                if len(collected_sentences) >= max_texts:
                    break
            
            if len(collected_sentences) >= max_texts:
                break
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§§ì€ ì§€ì—°
            time.sleep(0.1)
    
    except Exception as e:
        logger.error(f"âŒ ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
    
    logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: '{word}' ê´€ë ¨ {len(collected_sentences)}ê°œ ë¬¸ì¥")
    return collected_sentences

def collect_missing_words(**context):
    """ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´ë“¤ ìˆ˜ì§‘"""
    logger.info("=== ëª¨ë¸ ëˆ„ë½ ë‹¨ì–´ ìˆ˜ì§‘ ì‹œì‘ ===")
    
    try:
        # ë¨¼ì € ë””ë²„ê¹…: %ëª¨ë¸% ë§ˆì»¤ê°€ ìˆëŠ” ë ˆì½”ë“œ í™•ì¸
        try:
            with db_manager.get_connection() as conn:
                cursor = conn.cursor(dictionary=True)
                
                # %ëª¨ë¸% ë§ˆì»¤ê°€ ìˆëŠ” ë ˆì½”ë“œ ê°œìˆ˜ í™•ì¸
                debug_query = """
                SELECT COUNT(*) as count
                FROM game_detail
                WHERE description LIKE %s
                """
                cursor.execute(debug_query, (f'%{MODEL_CONFIG["missing_words_marker"]}%',))
                marker_count = cursor.fetchone()['count']
                logger.info(f"ğŸ” {MODEL_CONFIG['missing_words_marker']} ë§ˆì»¤ê°€ ìˆëŠ” ë ˆì½”ë“œ: {marker_count}ê°œ")
                
                # ìƒ˜í”Œ í™•ì¸
                if marker_count > 0:
                    sample_query = """
                    SELECT game_id, game_seq, answer_text, description
                    FROM game_detail
                    WHERE description LIKE %s
                    LIMIT 5
                    """
                    cursor.execute(sample_query, (f'%{MODEL_CONFIG["missing_words_marker"]}%',))
                    samples = cursor.fetchall()
                    logger.info(f"ğŸ“‹ ìƒ˜í”Œ ë ˆì½”ë“œë“¤:")
                    for sample in samples:
                        logger.info(f"  - {sample['game_id']}/{sample['game_seq']}: '{sample['answer_text']}' -> {sample['description']}")
                
                cursor.close()
                
        except Exception as debug_error:
            logger.error(f"ë””ë²„ê¹… ì¿¼ë¦¬ ì‹¤íŒ¨: {debug_error}")
        
        # %ëª¨ë¸% ë§ˆì»¤ê°€ ìˆëŠ” ê²Œì„ë“¤ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
        missing_words = db_manager.extract_training_words(BATCH_CONFIG['model_training_batch_size'])
        
        if not missing_words:
            logger.info("í•™ìŠµí•  ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {
                'collected_words': 0,
                'cleaned_words': 0,
                'words': []
            }
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°ë¡œ ë‹¨ì–´ ì •ì œ - ê¸°ì¡´ TextProcessor í™œìš©
        logger.info(f"ğŸ“ ë‹¨ì–´ ì •ì œ ì‹œì‘ (TextProcessor ì‚¬ìš©)...")
        
        # ë¹ ë¥¸ ë‹¨ì–´ ì •ì œ (KoNLPy ì—†ì´)
        logger.info(f"ğŸ“ ë¹ ë¥¸ ë‹¨ì–´ ì •ì œ ì‹œì‘...")
        
        cleaned_words = []
        seen = set()
        
        for word in missing_words:
            if not word:
                continue
                
            word = word.strip()
            
            # ë¹ ë¥¸ ê²€ì¦: í•œê¸€ë§Œ, 2-10ê¸€ì, ì¤‘ë³µ ì œê±°
            if (len(word) >= 2 and 
                len(word) <= 10 and 
                re.fullmatch(r'[ê°€-í£]+', word) and
                word not in seen):
                
                cleaned_words.append(word)
                seen.add(word)
        
        logger.info(f"âœ… ë¹ ë¥¸ ë‹¨ì–´ ì •ì œ ì™„ë£Œ: {len(missing_words)}ê°œ â†’ {len(cleaned_words)}ê°œ")
        
        # XComìœ¼ë¡œ ë‹¤ìŒ íƒœìŠ¤í¬ì— ì „ë‹¬
        context['task_instance'].xcom_push(key='missing_words', value=cleaned_words)
        
        return {
            'collected_words': len(missing_words),
            'cleaned_words': len(cleaned_words),
            'words': cleaned_words[:10]  # ë¡œê·¸ìš©ìœ¼ë¡œ ì²˜ìŒ 10ê°œë§Œ
        }
        
    except Exception as e:
        logger.error(f"ë‹¨ì–´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'collected_words': 0,
            'cleaned_words': 0,
            'words': [],
            'error': str(e)
        }

def retry_failed_games(**context):
    """ì‹¤íŒ¨/ëŒ€ê¸° ìƒíƒœì˜ ê²Œì„ë“¤ì„ ì¬ë¶„ì„"""
    logger.info("=== ì‹¤íŒ¨ ê²Œì„ ì¬ì²˜ë¦¬ ì‹œì‘ ===")
    
    try:
        # ì‹¤íŒ¨/ëŒ€ê¸° ìƒíƒœ ê²Œì„ë“¤ ì¡°íšŒ
        failed_games = db_manager.get_failed_games(BATCH_CONFIG['retry_games_batch_size'])
        
        if not failed_games:
            logger.info("ì¬ì²˜ë¦¬í•  ê²Œì„ì´ ì—†ìŠµë‹ˆë‹¤")
            return {
                'total_games': 0,
                'processed': 0,
                'failed': 0,
                'missing_words': 0
            }
        
        logger.info(f"ì¬ì²˜ë¦¬ ëŒ€ìƒ ê²Œì„ {len(failed_games)}ê°œ ë°œê²¬")
        
        # ê²Œì„ë“¤ì„ ì²˜ë¦¬ ì¤‘ ìƒíƒœë¡œ í‘œì‹œ
        for game in failed_games:
            db_manager.mark_as_processing(game['game_id'], game['game_seq'])
        
        # AI ì„œë¹„ìŠ¤ë¡œ ì¼ê´„ ë¶„ì„
        analysis_results = ai_client.batch_analyze_games(failed_games)
        
        # ê²°ê³¼ ì²˜ë¦¬
        stats = {
            'total_games': len(failed_games),
            'processed': 0,
            'failed': 0,
            'missing_words': 0
        }
        
        for result in analysis_results:
            game_id = result['game_id']
            game_seq = result['game_seq']
            
            if result['status'] == 'success':
                # AI ë¶„ì„ ì„±ê³µ - ê²°ê³¼ ì €ì¥
                if db_manager.update_game_ai_result(game_id, game_seq, result['ai_result']):
                    stats['processed'] += 1
                    logger.info(f"ê²Œì„ ì¬ì²˜ë¦¬ ì„±ê³µ: {game_id}/{game_seq}")
                else:
                    stats['failed'] += 1
                    logger.error(f"ê²Œì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {game_id}/{game_seq}")
                    
            elif result['status'] == 'missing_word':
                # ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´
                missing_word_desc = f"{result['answer_text']}ëŠ” {MODEL_CONFIG['missing_words_marker']}ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤"
                if db_manager.update_game_status(
                    game_id, game_seq, 
                    AI_STATUS_CODES['COMPLETED'],  # ì™„ë£Œë¡œ í‘œì‹œí•˜ë˜ ì„¤ëª…ì— ë§ˆì»¤ í¬í•¨
                    missing_word_desc
                ):
                    stats['missing_words'] += 1
                    logger.info(f"ëª¨ë¸ ëˆ„ë½ ë‹¨ì–´ í‘œì‹œ: {game_id}/{game_seq} - {result['answer_text']}")
                else:
                    stats['failed'] += 1
                    
            else:
                # ë¶„ì„ ì‹¤íŒ¨
                error_msg = result.get('error', 'AI ë¶„ì„ ì‹¤íŒ¨')
                if db_manager.mark_as_failed(game_id, game_seq, error_msg):
                    stats['failed'] += 1
                    logger.error(f"ê²Œì„ ì¬ì²˜ë¦¬ ì‹¤íŒ¨: {game_id}/{game_seq} - {error_msg}")
        
        logger.info(f"ì¬ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {stats['processed']}, ì‹¤íŒ¨ {stats['failed']}, ëˆ„ë½ë‹¨ì–´ {stats['missing_words']}")
        return stats
        
    except Exception as e:
        logger.error(f"ì¬ì²˜ë¦¬ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'total_games': 0,
            'processed': 0,
            'failed': 0,
            'missing_words': 0,
            'error': str(e)
        }

def update_trained_word_games(words: List[str]) -> int:
    """í•™ìŠµëœ ë‹¨ì–´ë“¤ì˜ ê²Œì„ì„ ì¬ë¶„ì„ ëŒ€ê¸° ìƒíƒœë¡œ ë³€ê²½"""
    updated_count = 0
    
    try:
        for word in words:
            # í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²Œì„ë“¤ ì°¾ê¸°
            with db_manager.get_connection() as conn:
                cursor = conn.cursor(dictionary=True)
                
                # %ëª¨ë¸% ë§ˆì»¤ê°€ ìˆê³  í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ëœ ê²Œì„ë“¤
                query = """
                SELECT game_id, game_seq
                FROM game_detail
                WHERE description LIKE %s
                AND answer_text = %s
                """
                
                cursor.execute(query, (f'%{MODEL_CONFIG["missing_words_marker"]}%', word))
                games = cursor.fetchall()
                
                # ëŒ€ê¸° ìƒíƒœë¡œ ë³€ê²½
                for game in games:
                    if db_manager.update_game_status(
                        game['game_id'], 
                        game['game_seq'],
                        AI_STATUS_CODES['WAITING'],
                        f'"{word}" ë‹¨ì–´ í•™ìŠµ ì™„ë£Œ - ì¬ë¶„ì„ ëŒ€ê¸° ì¤‘'
                    ):
                        updated_count += 1
                        logger.info(f"ì¬ë¶„ì„ ëŒ€ê¸°ë¡œ ë³€ê²½: {game['game_id']}/{game['game_seq']} - {word}")
                
                cursor.close()
                
    except Exception as e:
        logger.error(f"ê²Œì„ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    logger.info(f"ì´ {updated_count}ê°œ ê²Œì„ì„ ì¬ë¶„ì„ ëŒ€ê¸° ìƒíƒœë¡œ ë³€ê²½")
    return updated_count

def check_system_status(**context):
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    logger.info("=== ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ===")
    
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        db_healthy = db_manager.test_connection()
        
        # AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        ai_healthy = ai_client.check_health()
        
        # ì²˜ë¦¬ í†µê³„ ì¡°íšŒ
        stats = db_manager.get_processing_statistics()
        
        # ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        model_info = ai_client.get_model_info()
        
        status = {
            'database_healthy': db_healthy,
            'ai_service_healthy': ai_healthy,
            'processing_stats': stats,
            'model_info': model_info,
            'overall_healthy': db_healthy and ai_healthy
        }
        
        if status['overall_healthy']:
            logger.info("âœ… ì‹œìŠ¤í…œ ìƒíƒœ ì •ìƒ")
        else:
            logger.warning("âš ï¸ ì‹œìŠ¤í…œ ì¼ë¶€ êµ¬ì„±ìš”ì†Œì— ë¬¸ì œ ìˆìŒ")
        
        return status
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            'database_healthy': False,
            'ai_service_healthy': False,
            'overall_healthy': False,
            'error': str(e)
        }

# DAG ì •ì˜
memory_forest_compact_dag = DAG(
    'memory_forest_compact',
    default_args=dag_default_args,
    description='Memory Forest ì»´íŒ©íŠ¸ ë²„ì „ - í•µì‹¬ ê¸°ëŠ¥ë§Œ í¬í•¨',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ ë˜ëŠ” ì™¸ë¶€ íŠ¸ë¦¬ê±°
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'compact', 'ai-processing']
)

# ì‹¤íŒ¨ ê²Œì„ ì¬ì²˜ë¦¬ DAG
retry_dag = DAG(
    'memory_forest_retry_failed',
    default_args=dag_default_args,
    description='ì‹¤íŒ¨/ëŒ€ê¸° ê²Œì„ ì¬ì²˜ë¦¬',
    schedule_interval=SCHEDULES['retry_failed_games'],  # 10ë¶„ë§ˆë‹¤
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'retry', 'ai-processing']
)

# ëª¨ë¸ í•™ìŠµ DAG - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í¬í•¨
training_dag = DAG(
    'memory_forest_train_words',
    default_args=dag_default_args,
    description='ëª¨ë¸ì— ì—†ëŠ” ë‹¨ì–´ í•™ìŠµ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í¬í•¨',
    schedule_interval=SCHEDULES['train_missing_words'],  # ë§¤ì¼ ì˜¤ì „ 3ì‹œ
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'training', 'model', 'optuna']
)

# íƒœìŠ¤í¬ ì •ì˜ - ì¬ì²˜ë¦¬ DAG
start_retry = DummyOperator(task_id='start_retry', dag=retry_dag)

check_status_task = PythonOperator(
    task_id='check_system_status',
    python_callable=check_system_status,
    dag=retry_dag
)

retry_games_task = PythonOperator(
    task_id='retry_failed_games',
    python_callable=retry_failed_games,
    dag=retry_dag
)

end_retry = DummyOperator(task_id='end_retry', dag=retry_dag)

# íƒœìŠ¤í¬ ì •ì˜ - í•™ìŠµ DAG (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í¬í•¨)
start_training = DummyOperator(task_id='start_training', dag=training_dag)

collect_words_task = PythonOperator(
    task_id='collect_missing_words',
    python_callable=collect_missing_words,
    dag=training_dag
)

train_words_task = PythonOperator(
    task_id='train_missing_words',
    python_callable=train_missing_words,
    dag=training_dag,
    priority_weight=10  # ë†’ì€ ìš°ì„ ìˆœìœ„
)

end_training = DummyOperator(task_id='end_training', dag=training_dag)

# ì˜ì¡´ì„± ì„¤ì •
start_retry >> check_status_task >> retry_games_task >> end_retry
start_training >> collect_words_task >> train_words_task >> end_training