"""
Memory Forest ì‹¤íŒ¨ ë‹¨ì–´ ìˆ˜ì§‘ ë° í•™ìŠµ ë°ì´í„° ìƒì„± DAG - ê¸°ì¡´ ì½”ë“œ í˜¸í™˜
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime, timedelta
import pendulum
import logging
import urllib.request
import urllib.parse
import json
import os
import re
import time
from typing import List, Dict

# ë¡œì»¬ ëª¨ë“ˆ import
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import DAG_DEFAULT_ARGS, SCHEDULES, DEFAULT_ARGS, LOCAL_TZ, API_CONFIG
from utils.database import db_manager

logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ì—ì„œ Naver API ì •ë³´ ê°€ì ¸ì˜¤ê¸°
NAVER_CLIENT_ID = API_CONFIG['naver_client_id']
NAVER_CLIENT_SECRET = API_CONFIG['naver_client_secret']

def collect_failed_words(**context):
    """ëª¨ë¸ì— ì—†ì–´ì„œ ì‹¤íŒ¨í•œ ë‹¨ì–´ë“¤ì„ DBì—ì„œ ìˆ˜ì§‘"""
    logger.info("=== ì‹¤íŒ¨ ë‹¨ì–´ ìˆ˜ì§‘ ì‹œì‘ ===")
    
    try:
        with db_manager.get_connection() as conn:
            cursor = conn.cursor(buffered=True, dictionary=True)
            
            # ìµœê·¼ 7ì¼ê°„ "ëª¨ë¸ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ì˜¤ë¥˜ë¡œ ì‹¤íŒ¨í•œ ë‹¨ì–´ë“¤ ìˆ˜ì§‘
            query = """
            SELECT 
                answer_text,
                COUNT(*) as fail_count,
                MAX(ai_processed_at) as last_failure,
                MIN(ai_processed_at) as first_failure
            FROM game_detail 
            WHERE ai_status_code = 'B20008'
            AND description LIKE '%ëª¨ë¸ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤%'
            AND ai_processed_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)
            AND answer_text IS NOT NULL 
            AND answer_text != ''
            AND LENGTH(answer_text) >= 2  -- ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
            AND LENGTH(answer_text) <= 10 -- ë„ˆë¬´ ê¸´ ë‹¨ì–´ ì œì™¸
            GROUP BY answer_text
            HAVING fail_count >= 1  
            ORDER BY fail_count DESC, last_failure DESC
            LIMIT 50  -- ìµœëŒ€ 50ê°œ ë‹¨ì–´ë§Œ ì²˜ë¦¬
            """
            
            cursor.execute(query)
            failed_words = cursor.fetchall()
            
            # ì•ˆì „í•œ ì»¤ì„œ ë‹«ê¸°
            try:
                while cursor.nextset():
                    pass
            except:
                pass
            cursor.close()
            
            if not failed_words:
                logger.info("ğŸ“­ ìˆ˜ì§‘í•  ì‹¤íŒ¨ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {"collected_words": [], "total_count": 0}
            
            # ê²°ê³¼ ì €ì¥
            today = datetime.now(LOCAL_TZ).strftime("%Y%m%d")
            failed_words_dir = f"/opt/airflow/data/failed_words/{today}"
            os.makedirs(failed_words_dir, exist_ok=True)
            
            failed_words_file = os.path.join(failed_words_dir, "failed_words.json")
            with open(failed_words_file, 'w', encoding='utf-8') as f:
                json.dump(failed_words, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ“Š ì‹¤íŒ¨ ë‹¨ì–´ ìˆ˜ì§‘ ì™„ë£Œ: {len(failed_words)}ê°œ")
            for word in failed_words[:10]:  # ìƒìœ„ 10ê°œë§Œ ë¡œê¹…
                logger.info(f"  '{word['answer_text']}': {word['fail_count']}íšŒ ì‹¤íŒ¨")
            
            # XComìœ¼ë¡œ ì „ë‹¬
            context['task_instance'].xcom_push(
                key='failed_words_file', 
                value=failed_words_file
            )
            context['task_instance'].xcom_push(
                key='failed_words_count', 
                value=len(failed_words)
            )
            
            return {
                "collected_words": [w['answer_text'] for w in failed_words],
                "total_count": len(failed_words),
                "file_path": failed_words_file
            }
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤íŒ¨ ë‹¨ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {"collected_words": [], "total_count": 0, "error": str(e)}

def getRequestUrl(url: str) -> str:
    """Naver API ìš”ì²­"""
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.error("âŒ Naver API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
    
    try:
        response = urllib.request.urlopen(req)
        if response.getcode() == 200:
            return response.read().decode('utf-8')
    except Exception as e:
        logger.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {url} - {e}")
        return None

def getNaverSearch(node: str, query: str, start: int = 1, display: int = 100) -> Dict:
    """Naver ê²€ìƒ‰ API"""
    base = "https://openapi.naver.com/v1/search"
    node_path = f"/{node}.json"
    parameters = f"?query={urllib.parse.quote(query)}&start={start}&display={display}"
    url = base + node_path + parameters
    
    response = getRequestUrl(url)
    return json.loads(response) if response else None

def clean_html(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<.*?>', '', text)
    # HTML ì—”í‹°í‹° ì œê±°
    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)
    # íŠ¹ìˆ˜ë¬¸ì ë° ê¸°í˜¸ ì •ë¦¬
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def collect_word_contexts(**context):
    """ì‹¤íŒ¨í•œ ë‹¨ì–´ë“¤ì˜ ë§¥ë½ í…ìŠ¤íŠ¸ë¥¼ Naver APIë¡œ ìˆ˜ì§‘"""
    logger.info("=== ë‹¨ì–´ ë§¥ë½ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘ ===")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ì‹¤íŒ¨ ë‹¨ì–´ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    failed_words_file = context['task_instance'].xcom_pull(
        task_ids='collect_failed_words',
        key='failed_words_file'
    )
    failed_words_count = context['task_instance'].xcom_pull(
        task_ids='collect_failed_words',
        key='failed_words_count'
    )
    
    if not failed_words_file or not os.path.exists(failed_words_file):
        logger.warning("âš ï¸ ì‹¤íŒ¨ ë‹¨ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"collected_texts": 0, "total_words": 0}
    
    if failed_words_count == 0:
        logger.info("ğŸ“­ ì²˜ë¦¬í•  ì‹¤íŒ¨ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"collected_texts": 0, "total_words": 0}
    
    # API í‚¤ í™•ì¸
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.error("âŒ Naver API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {"collected_texts": 0, "total_words": 0, "error": "API í‚¤ ì—†ìŒ"}
    
    # ì‹¤íŒ¨ ë‹¨ì–´ ë¡œë“œ
    with open(failed_words_file, 'r', encoding='utf-8') as f:
        failed_words = json.load(f)
    
    today = datetime.now(LOCAL_TZ).strftime("%Y%m%d")
    contexts_dir = f"/opt/airflow/data/collected_texts/{today}"
    os.makedirs(contexts_dir, exist_ok=True)
    
    total_texts = 0
    successful_words = 0
    
    for word_info in failed_words:
        word = word_info['answer_text']
        logger.info(f"ğŸ” '{word}' ê´€ë ¨ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # ë¸”ë¡œê·¸ ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ì–‘í•œ ë§¥ë½ì˜ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            search_result = getNaverSearch('blog', word, 1, 50)  # 50ê°œì”© ìˆ˜ì§‘
            
            if not search_result or 'items' not in search_result:
                logger.warning(f"âš ï¸ '{word}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue
            
            word_texts = []
            for item in search_result['items']:
                title = clean_html(item.get('title', ''))
                description = clean_html(item.get('description', ''))
                
                # ì œëª©ê³¼ ë³¸ë¬¸ì„ í•©ì³ì„œ ì €ì¥
                full_text = f"{title} {description}".strip()
                
                # í’ˆì§ˆ í•„í„°ë§
                if (full_text and 
                    len(full_text) > 20 and  # ìµœì†Œ 20ì
                    len(full_text) < 1000 and  # ìµœëŒ€ 1000ì
                    word in full_text):  # ì‹¤ì œë¡œ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    
                    word_texts.append({
                        'word': word,
                        'text': full_text,
                        'source': 'naver_blog',
                        'title': title,
                        'fail_count': word_info['fail_count'],
                        'collected_at': datetime.now(LOCAL_TZ).isoformat()
                    })
            
            # ë‹¨ì–´ë³„ë¡œ íŒŒì¼ ì €ì¥
            if word_texts:
                word_file = os.path.join(contexts_dir, f"{word}_contexts.json")
                with open(word_file, 'w', encoding='utf-8') as f:
                    json.dump(word_texts, f, indent=2, ensure_ascii=False)
                
                total_texts += len(word_texts)
                successful_words += 1
                logger.info(f"âœ… '{word}': {len(word_texts)}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ '{word}': ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
            
            # API ìš”ì²­ ì œí•œ ì¤€ìˆ˜ (ì´ˆë‹¹ 10íšŒ ì œí•œ)
            time.sleep(0.1)
            
        except Exception as e:
            logger.error(f"âŒ '{word}' í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    # í†µí•© íŒŒì¼ ìƒì„± (ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ)
    all_texts_file = os.path.join(contexts_dir, "all_collected_texts.txt")
    with open(all_texts_file, 'w', encoding='utf-8') as f:
        for filename in os.listdir(contexts_dir):
            if filename.endswith('_contexts.json'):
                filepath = os.path.join(contexts_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as json_file:
                    texts = json.load(json_file)
                    for text_data in texts:
                        f.write(text_data['text'] + '\n')
    
    logger.info(f"ğŸ‰ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {successful_words}/{len(failed_words)}ê°œ ë‹¨ì–´, ì´ {total_texts}ê°œ í…ìŠ¤íŠ¸")
    
    # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
    context['task_instance'].xcom_push(
        key='collected_texts_file', 
        value=all_texts_file
    )
    context['task_instance'].xcom_push(
        key='collected_texts_count', 
        value=total_texts
    )
    
    return {
        "collected_texts": total_texts,
        "successful_words": successful_words,
        "total_words": len(failed_words),
        "texts_file": all_texts_file
    }

def prepare_training_data(**context):
    """ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ë¥¼ Word2Vec í•™ìŠµìš© ë°ì´í„°ë¡œ ì „ì²˜ë¦¬"""
    logger.info("=== í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    texts_file = context['task_instance'].xcom_pull(
        task_ids='collect_word_contexts',
        key='collected_texts_file'
    )
    texts_count = context['task_instance'].xcom_pull(
        task_ids='collect_word_contexts',
        key='collected_texts_count'
    )
    
    if not texts_file or not os.path.exists(texts_file) or texts_count == 0:
        logger.warning("âš ï¸ ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"processed_sentences": 0, "training_ready": False}
    
    try:
        # KoNLPy í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ import
        from konlpy.tag import Okt
        okt = Okt()
        
        # í…ìŠ¤íŠ¸ ì½ê¸°
        with open(texts_file, 'r', encoding='utf-8') as f:
            texts = f.readlines()
        
        logger.info(f"ğŸ“– ì›ë³¸ í…ìŠ¤íŠ¸ {len(texts)}ì¤„ ë¡œë“œ ì™„ë£Œ")
        
        # í˜•íƒœì†Œ ë¶„ì„ ë° ì „ì²˜ë¦¬
        processed_sentences = []
        
        for i, text in enumerate(texts):
            text = text.strip()
            if len(text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                continue
                
            try:
                # í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ)
                pos_tags = okt.pos(text, stem=True, norm=True)
                words = [word for word, pos in pos_tags 
                        if pos in ['Noun', 'Verb', 'Adjective'] 
                        and len(word) >= 2]  # 2ê¸€ì ì´ìƒë§Œ
                
                if len(words) >= 3:  # ìµœì†Œ 3ê°œ ë‹¨ì–´ ì´ìƒ
                    processed_sentences.append(words)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ ({i+1}ë²ˆì§¸): {e}")
                continue
                
            # ì§„í–‰ìƒí™© ë¡œê¹…
            if (i + 1) % 100 == 0:
                logger.info(f"ğŸ“Š ì§„í–‰ìƒí™©: {i+1}/{len(texts)} ì²˜ë¦¬ ì™„ë£Œ")
        
        if not processed_sentences:
            logger.error("âŒ ì „ì²˜ë¦¬ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"processed_sentences": 0, "training_ready": False}
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        today = datetime.now(LOCAL_TZ).strftime("%Y%m%d")
        training_dir = f"/opt/airflow/data/model_training/{today}"
        os.makedirs(training_dir, exist_ok=True)
        
        training_data_file = os.path.join(training_dir, "processed_sentences.json")
        with open(training_data_file, 'w', encoding='utf-8') as f:
            json.dump(processed_sentences, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_sentences)}ê°œ ë¬¸ì¥")
        
        # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
        context['task_instance'].xcom_push(
            key='training_data_file', 
            value=training_data_file
        )
        context['task_instance'].xcom_push(
            key='processed_sentences_count', 
            value=len(processed_sentences)
        )
        
        return {
            "processed_sentences": len(processed_sentences),
            "training_ready": True,
            "training_data_file": training_data_file
        }
        
    except ImportError:
        logger.error("âŒ KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. requirements.txtì— konlpyë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return {"processed_sentences": 0, "training_ready": False, "error": "KoNLPy ì—†ìŒ"}
    except Exception as e:
        logger.error(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"processed_sentences": 0, "training_ready": False, "error": str(e)}

# DAG ì •ì˜
word_collector_default_args = {
    **DEFAULT_ARGS,
    'start_date': datetime(2024, 1, 1, tzinfo=LOCAL_TZ),
    'retries': 2,  # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ë¹„ ì¬ì‹œë„
    'retry_delay': timedelta(minutes=5),
}

word_collector_dag = DAG(
    'memory_forest_word_collector',
    default_args=word_collector_default_args,
    description='Memory Forest ì‹¤íŒ¨ ë‹¨ì–´ ìˆ˜ì§‘ ë° í•™ìŠµ ë°ì´í„° ìƒì„±',
    schedule_interval=SCHEDULES['word_collection'],  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'word2vec', 'data-collection', 'naver-api']
)

# Task ì •ì˜
start_collection = DummyOperator(
    task_id='start_collection',
    dag=word_collector_dag
)

collect_failed_words_task = PythonOperator(
    task_id='collect_failed_words',
    python_callable=collect_failed_words,
    dag=word_collector_dag
)

collect_contexts_task = PythonOperator(
    task_id='collect_word_contexts',
    python_callable=collect_word_contexts,
    dag=word_collector_dag
)

prepare_training_task = PythonOperator(
    task_id='prepare_training_data',
    python_callable=prepare_training_data,
    dag=word_collector_dag
)

end_collection = DummyOperator(
    task_id='end_collection',
    dag=word_collector_dag
)

# Task ì˜ì¡´ì„±
start_collection >> collect_failed_words_task >> collect_contexts_task >> prepare_training_task >> end_collection