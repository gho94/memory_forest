"""
Memory Forest ëª¨ë‹ˆí„°ë§ DAG
ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§, ì•Œë¦¼, ê¸´ê¸‰ ìƒí™© ëŒ€ì‘
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import pendulum
import logging

# ë¡œì»¬ ëª¨ë“ˆ import
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import DAG_DEFAULT_ARGS, MONITORING_CONFIG, STATUS_CODES
from utils.database import db_manager
from utils.ai_service import ai_client

local_tz = pendulum.timezone("Asia/Seoul")
logger = logging.getLogger(__name__)

def check_critical_metrics(**context):
    """í•µì‹¬ ì§€í‘œ ëª¨ë‹ˆí„°ë§"""
    logger.info("=== í•µì‹¬ ì§€í‘œ ì²´í¬ ì‹œì‘ ===")
    
    alerts = []
    metrics = {
        "ai_service_healthy": False,
        "database_healthy": False,
        "pending_games_count": 0,
        "error_rate_today": 0.0,
        "processing_queue_health": "unknown"
    }
    
    try:
        # AI ì„œë¹„ìŠ¤ ìƒíƒœ
        metrics["ai_service_healthy"] = ai_client.check_health()
        if not metrics["ai_service_healthy"]:
            alerts.append("AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì—†ìŒ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë° í†µê³„
        try:
            stats = db_manager.get_processing_statistics()
            metrics["database_healthy"] = True
            
            # ëŒ€ê¸° ì¤‘ì¸ ê²Œì„ ìˆ˜ ì²´í¬
            pending_count = 0
            for stat in stats['status_breakdown']:
                if stat['ai_status_code'] == STATUS_CODES['PENDING']:
                    pending_count = stat['total_count']
                    break
            
            metrics["pending_games_count"] = pending_count
            
            # ì„ê³„ì¹˜ ì²´í¬
            if pending_count > MONITORING_CONFIG['pending_threshold_count']:
                alerts.append(f"ëŒ€ê¸° ê²Œì„ ìˆ˜ ì„ê³„ì¹˜ ì´ˆê³¼: {pending_count}ê°œ")
            
            # ì˜¤ëŠ˜ì˜ ì˜¤ë¥˜ìœ¨ ê³„ì‚°
            today_stats = stats['today_summary']
            if today_stats['total_processed_today'] > 0:
                error_rate = (today_stats['failed_today'] / today_stats['total_processed_today']) * 100
                metrics["error_rate_today"] = round(error_rate, 2)
                
                if error_rate > MONITORING_CONFIG['error_threshold_percentage']:
                    alerts.append(f"ì˜¤ë¥˜ìœ¨ ì„ê³„ì¹˜ ì´ˆê³¼: {error_rate:.1f}%")
            
            # ì²˜ë¦¬ ëŒ€ê¸°ì—´ ìƒíƒœ íŒë‹¨
            if pending_count == 0:
                metrics["processing_queue_health"] = "idle"
            elif pending_count < 50:
                metrics["processing_queue_health"] = "normal"
            elif pending_count < 100:
                metrics["processing_queue_health"] = "busy"
            else:
                metrics["processing_queue_health"] = "overloaded"
                
        except Exception as e:
            alerts.append(f"ë°ì´í„°ë² ì´ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {str(e)}")
            metrics["database_healthy"] = False
        
        # ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        logger.info("=== ì‹œìŠ¤í…œ í•µì‹¬ ì§€í‘œ ===")
        logger.info(f"AI ì„œë¹„ìŠ¤: {'ì •ìƒ' if metrics['ai_service_healthy'] else 'ë¹„ì •ìƒ'}")
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤: {'ì •ìƒ' if metrics['database_healthy'] else 'ë¹„ì •ìƒ'}")
        logger.info(f"ëŒ€ê¸° ê²Œì„ ìˆ˜: {metrics['pending_games_count']}ê°œ")
        logger.info(f"ì˜¤ëŠ˜ ì˜¤ë¥˜ìœ¨: {metrics['error_rate_today']}%")
        logger.info(f"ì²˜ë¦¬ ëŒ€ê¸°ì—´: {metrics['processing_queue_health']}")
        
        if alerts:
            logger.warning("=== ì•Œë¦¼ ë°œìƒ ===")
            for alert in alerts:
                logger.warning(f"  âš ï¸  {alert}")
        else:
            logger.info("âœ… ëª¨ë“  ì§€í‘œ ì •ìƒ")
        
        return {
            "metrics": metrics,
            "alerts": alerts,
            "status": "critical" if alerts else "normal"
        }
        
    except Exception as e:
        logger.error(f"í•µì‹¬ ì§€í‘œ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "metrics": metrics,
            "alerts": [f"ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}"],
            "status": "error"
        }

def check_disk_space(**context):
    """ë””ìŠ¤í¬ ê³µê°„ ì²´í¬"""
    logger.info("=== ë””ìŠ¤í¬ ê³µê°„ ì²´í¬ ===")
    
    try:
        # df ëª…ë ¹ì–´ë¡œ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        import subprocess
        result = subprocess.run(['df', '-h', '/opt/airflow'], capture_output=True, text=True)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            if len(lines) > 1:
                parts = lines[1].split()
                if len(parts) >= 5:
                    usage_percent = parts[4].rstrip('%')
                    logger.info(f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {usage_percent}%")
                    
                    if int(usage_percent) > 85:
                        logger.warning(f"ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {usage_percent}% ì‚¬ìš©ì¤‘")
                        return {"disk_status": "warning", "usage": f"{usage_percent}%"}
                    else:
                        return {"disk_status": "normal", "usage": f"{usage_percent}%"}
        
        return {"disk_status": "unknown", "usage": "í™•ì¸ ë¶ˆê°€"}
        
    except Exception as e:
        logger.error(f"ë””ìŠ¤í¬ ê³µê°„ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {"disk_status": "error", "error": str(e)}

def check_memory_usage(**context):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
    logger.info("=== ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ ===")
    
    try:
        import psutil
        
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_percent:.1f}%")
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {memory.available / (1024**3):.1f}GB")
        
        if memory_percent > 90:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_percent:.1f}%")
            return {"memory_status": "critical", "usage": f"{memory_percent:.1f}%"}
        elif memory_percent > 80:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜: {memory_percent:.1f}%")
            return {"memory_status": "warning", "usage": f"{memory_percent:.1f}%"}
        else:
            return {"memory_status": "normal", "usage": f"{memory_percent:.1f}%"}
            
    except ImportError:
        logger.info("psutil ëª¨ë“ˆ ì—†ìŒ - ë©”ëª¨ë¦¬ ì²´í¬ ìŠ¤í‚µ")
        return {"memory_status": "skipped", "reason": "psutil not available"}
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì²´í¬ ì‹¤íŒ¨: {e}")
        return {"memory_status": "error", "error": str(e)}

def emergency_response(**context):
    """ê¸´ê¸‰ ìƒí™© ëŒ€ì‘"""
    logger.info("=== ê¸´ê¸‰ ìƒí™© ì²´í¬ ===")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë©”íŠ¸ë¦­ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    metrics_result = context['task_instance'].xcom_pull(task_ids='check_critical_metrics')
    
    if not metrics_result:
        logger.error("ë©”íŠ¸ë¦­ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
        return {"emergency_actions": [], "status": "error"}
    
    emergency_actions = []
    
    try:
        alerts = metrics_result.get('alerts', [])
        metrics = metrics_result.get('metrics', {})
        
        # ê¸´ê¸‰ ìƒí™© íŒë‹¨ ë° ëŒ€ì‘
        if not metrics.get('ai_service_healthy'):
            emergency_actions.append("AI ì„œë¹„ìŠ¤ ì¬ì‹œì‘ í•„ìš”")
            
        if not metrics.get('database_healthy'):
            emergency_actions.append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë³µêµ¬ í•„ìš”")
            
        if metrics.get('pending_games_count', 0) > 500:
            emergency_actions.append("ëŒ€ê¸° ê²Œì„ ìˆ˜ ê³¼ë‹¤ - ì²˜ë¦¬ ëŠ¥ë ¥ ì¦ëŒ€ í•„ìš”")
            
        if metrics.get('error_rate_today', 0) > 50:
            emergency_actions.append("ë†’ì€ ì˜¤ë¥˜ìœ¨ - ì‹œìŠ¤í…œ ì ê²€ í•„ìš”")
        
        # ê¸´ê¸‰ ìƒí™© ì‹œ ìë™ ëŒ€ì‘ (ì˜ˆì‹œ)
        if emergency_actions:
            logger.warning("=== ê¸´ê¸‰ ìƒí™© ê°ì§€ ===")
            for action in emergency_actions:
                logger.warning(f"ğŸš¨ {action}")
            
            # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì—¬ê¸°ì— ìë™í™”ëœ ë³µêµ¬ ì•¡ì…˜ ì¶”ê°€
            # ì˜ˆ: ì„œë¹„ìŠ¤ ì¬ì‹œì‘, ì•Œë¦¼ ë°œì†¡, ë¶€í•˜ ë¶„ì‚° ë“±
            
        else:
            logger.info("âœ… ê¸´ê¸‰ ìƒí™© ì—†ìŒ")
        
        return {
            "emergency_actions": emergency_actions,
            "status": "emergency" if emergency_actions else "normal"
        }
        
    except Exception as e:
        logger.error(f"ê¸´ê¸‰ ìƒí™© ì²´í¬ ì‹¤íŒ¨: {e}")
        return {"emergency_actions": [], "status": "error", "error": str(e)}

# ëª¨ë‹ˆí„°ë§ DAG ì •ì˜
monitoring_default_args = {
    **DAG_DEFAULT_ARGS,
    'start_date': datetime(2024, 1, 1, tzinfo=local_tz),  # start_date ì¶”ê°€
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

monitoring_dag = DAG(
    'memory_forest_monitoring',
    default_args=monitoring_default_args,
    description='Memory Forest ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼',
    schedule_interval='*/15 * * * *',  # 15ë¶„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'monitoring', 'alerts']
)

# ëª¨ë‹ˆí„°ë§ Task ì •ì˜
metrics_check_task = PythonOperator(
    task_id='check_critical_metrics',
    python_callable=check_critical_metrics,
    dag=monitoring_dag
)

disk_check_task = PythonOperator(
    task_id='check_disk_space',
    python_callable=check_disk_space,
    dag=monitoring_dag
)

memory_check_task = PythonOperator(
    task_id='check_memory_usage',
    python_callable=check_memory_usage,
    dag=monitoring_dag
)

emergency_task = PythonOperator(
    task_id='emergency_response',
    python_callable=emergency_response,
    dag=monitoring_dag
)

# ëª¨ë‹ˆí„°ë§ DAG ì˜ì¡´ì„±
metrics_check_task >> [disk_check_task, memory_check_task] >> emergency_task