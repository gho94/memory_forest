"""
Memory Forest Word2Vec ëª¨ë¸ ì¬í•™ìŠµ DAG
ìˆ˜ì§‘ëœ í•™ìŠµ ë°ì´í„°ë¡œ Word2Vec ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ê³  ì„±ëŠ¥ ë¹„êµ í›„ êµì²´
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta
import pendulum
import logging
import json
import os
import shutil
import time
import csv
import requests
from typing import Dict, List

# ë¡œì»¬ ëª¨ë“ˆ import
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import DAG_DEFAULT_ARGS, AI_SERVICE_CONFIG
from utils.database import db_manager

local_tz = pendulum.timezone("Asia/Seoul")
logger = logging.getLogger(__name__)

# AI ì„œë¹„ìŠ¤ URL (.env íŒŒì¼ì—ì„œ ê°€ì ¸ì˜´)
AI_SERVICE_URL = os.getenv("AI_SERVICE_URL", "http://ai-service:8000")

def check_training_data_available(**context):
    """í•™ìŠµ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    logger.info("=== í•™ìŠµ ë°ì´í„° ê°€ìš©ì„± í™•ì¸ ===")
    
    today = datetime.now(local_tz).strftime("%Y%m%d")
    training_data_file = f"/opt/airflow/data/model_training/{today}/processed_sentences.json"
    
    if not os.path.exists(training_data_file):
        logger.warning(f"âš ï¸ í•™ìŠµ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {training_data_file}")
        return {"training_ready": False, "reason": "í•™ìŠµ ë°ì´í„° íŒŒì¼ ì—†ìŒ"}
    
    try:
        with open(training_data_file, 'r', encoding='utf-8') as f:
            sentences = json.load(f)
        
        if len(sentences) < 100:  # ìµœì†Œ 100ê°œ ë¬¸ì¥ í•„ìš”
            logger.warning(f"âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(sentences)}ê°œ ë¬¸ì¥")
            return {"training_ready": False, "reason": f"ë°ì´í„° ë¶€ì¡± ({len(sentences)}ê°œ)"}
        
        logger.info(f"âœ… í•™ìŠµ ë°ì´í„° í™•ì¸ ì™„ë£Œ: {len(sentences)}ê°œ ë¬¸ì¥")
        
        # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
        context['task_instance'].xcom_push(
            key='training_data_file', 
            value=training_data_file
        )
        context['task_instance'].xcom_push(
            key='sentences_count', 
            value=len(sentences)
        )
        
        return {
            "training_ready": True,
            "sentences_count": len(sentences),
            "data_file": training_data_file
        }
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
        return {"training_ready": False, "reason": f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"}

def train_enhanced_word2vec(**context):
    """ê¸°ì¡´ ëª¨ë¸ + ìƒˆ ë°ì´í„°ë¡œ Word2Vec ëª¨ë¸ ì¬í•™ìŠµ"""
    logger.info("=== Word2Vec ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘ ===")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ í•™ìŠµ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    training_data_file = context['task_instance'].xcom_pull(
        task_ids='check_training_data',
        key='training_data_file'
    )
    sentences_count = context['task_instance'].xcom_pull(
        task_ids='check_training_data',
        key='sentences_count'
    )
    
    if not training_data_file:
        logger.error("âŒ í•™ìŠµ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"training_success": False, "reason": "í•™ìŠµ ë°ì´í„° ì—†ìŒ"}
    
    try:
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
        import numpy as np
        import optuna
        from gensim.models import Word2Vec
        from numpy import dot
        from numpy.linalg import norm
        
        start_time = time.time()
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        current_model_path = "/opt/airflow/models/word2vec_custom.model"
        backup_model_path = "/opt/airflow/models/word2vec_custom_backup.model"
        new_model_path = "/opt/airflow/models/word2vec_custom_new.model"
        
        # ê¸°ì¡´ ëª¨ë¸ ë°±ì—…
        if os.path.exists(current_model_path):
            if os.path.exists(backup_model_path):
                if os.path.isdir(backup_model_path):
                    shutil.rmtree(backup_model_path)
                else:
                    os.remove(backup_model_path)
            shutil.copy2(current_model_path, backup_model_path)
            logger.info("ğŸ“¦ ê¸°ì¡´ ëª¨ë¸ ë°±ì—… ì™„ë£Œ")
        
        # ìƒˆ í•™ìŠµ ë°ì´í„° ë¡œë“œ
        with open(training_data_file, 'r', encoding='utf-8') as f:
            new_sentences = json.load(f)
        
        logger.info(f"ğŸ“š ìƒˆ í•™ìŠµ ë°ì´í„° ë¡œë“œ: {len(new_sentences)}ê°œ ë¬¸ì¥")
        
        # ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ê²°í•©
        all_sentences = new_sentences.copy()
        
        if os.path.exists(current_model_path):
            try:
                existing_model = Word2Vec.load(current_model_path)
                logger.info(f"ğŸ“– ê¸°ì¡´ ëª¨ë¸ ì–´íœ˜ í¬ê¸°: {len(existing_model.wv)}")
                
                # ê¸°ì¡´ ëª¨ë¸ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´
                # ê¸°ì¡´ ì–´íœ˜ë¡œ ê°€ìƒì˜ ë¬¸ì¥ ìƒì„± (ì„ íƒì )
                existing_vocab = list(existing_model.wv.key_to_index.keys())
                
                # ê¸°ì¡´ ì–´íœ˜ë¥¼ í¬í•¨í•œ ë¬¸ì¥ë“¤ì„ ì¼ë¶€ ì¶”ê°€ (ë‹¤ì–‘ì„± í™•ë³´)
                for i in range(min(100, len(existing_vocab) // 10)):
                    if i * 10 + 10 <= len(existing_vocab):
                        synthetic_sentence = existing_vocab[i*10:(i*10)+10]
                        all_sentences.append(synthetic_sentence)
                
                logger.info(f"ğŸ“ˆ ê¸°ì¡´ ì–´íœ˜ í†µí•© ì™„ë£Œ: ì´ {len(all_sentences)}ê°œ ë¬¸ì¥")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ í•™ìŠµ: {e}")
        
        # ì„±ëŠ¥ í‰ê°€ìš© ê¸°ì¤€ ë‹¨ì–´ë“¤ (ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬)
        test_words = [
            # ê°€ì¡± ê´€ë ¨
            "ë¶€ëª¨", "ì•„ë²„ì§€", "ì–´ë¨¸ë‹ˆ", "ì•„ë“¤", "ë”¸", "í˜•ì œ", "ìë§¤", "í• ë¨¸ë‹ˆ", "í• ì•„ë²„ì§€",
            # ê°ì • ê´€ë ¨  
            "ê¸°ì¨", "ìŠ¬í””", "ì‚¬ë‘", "í–‰ë³µ", "ê±±ì •", "ë‘ë ¤ì›€", "í¬ë§", "ê°ë™", "ê·¸ë¦¬ì›€",
            # ì¼ìƒ ê´€ë ¨
            "ìŒì‹", "ì§‘", "í•™êµ", "ë³‘ì›", "íšŒì‚¬", "ì¹œêµ¬", "ì„ ìƒë‹˜", "ì˜ì‚¬", "ê°„í˜¸ì‚¬",
            # ë™ë¬¼ ê´€ë ¨
            "ê°•ì•„ì§€", "ê³ ì–‘ì´", "ìƒˆ", "ë¬¼ê³ ê¸°", "í† ë¼", "í–„ìŠ¤í„°", "ê±°ë¶ì´", "ì•µë¬´ìƒˆ",
            # ê³„ì ˆ/ìì—° ê´€ë ¨
            "ë´„", "ì—¬ë¦„", "ê°€ì„", "ê²¨ìš¸", "ë°”ë‹¤", "ì‚°", "ê°•", "í•˜ëŠ˜", "êµ¬ë¦„", "ë³„"
        ]
        
        def calculate_avg_similarity(model, words):
            """í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
            vectors = []
            for word in words:
                if word in model.wv:
                    vectors.append(model.wv[word])
            
            if len(vectors) < 2:
                return 0.0
            
            similarities = []
            for i in range(len(vectors)):
                for j in range(i + 1, len(vectors)):
                    sim = dot(vectors[i], vectors[j]) / (norm(vectors[i]) * norm(vectors[j]))
                    similarities.append(sim)
            
            return round(np.mean(similarities), 4) if similarities else 0.0
        
        # ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        baseline_similarity = 0.0
        if os.path.exists(current_model_path):
            try:
                baseline_model = Word2Vec.load(current_model_path)
                baseline_similarity = calculate_avg_similarity(baseline_model, test_words)
                logger.info(f"ğŸ“Š ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ (í‰ê·  ìœ ì‚¬ë„): {baseline_similarity}")
            except:
                logger.warning("âš ï¸ ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
        
        # Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        def objective(trial):
            vector_size = trial.suggest_categorical("vector_size", [100, 150, 200])
            window = trial.suggest_int("window", 5, 10)
            min_count = trial.suggest_int("min_count", 2, 5)
            epochs = trial.suggest_int("epochs", 10, 20)
            alpha = trial.suggest_float("alpha", 0.01, 0.05)
            
            try:
                model = Word2Vec(
                    sentences=all_sentences,
                    vector_size=vector_size,
                    window=window,
                    min_count=min_count,
                    workers=4,
                    sg=1,  # Skip-gram
                    epochs=epochs,
                    alpha=alpha,
                    seed=42
                )
                
                score = calculate_avg_similarity(model, test_words)
                return score
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œë„ ì‹¤íŒ¨: {e}")
                return 0.0
        
        # ìµœì í™” ì‹¤í–‰
        logger.info("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        study = optuna.create_study(direction="maximize")
        study.optimize(objective, n_trials=15, timeout=300)  # 5ë¶„ ì œí•œ
        
        best_params = study.best_params
        logger.info(f"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        logger.info("ğŸš€ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        final_model = Word2Vec(
            sentences=all_sentences,
            vector_size=best_params["vector_size"],
            window=best_params["window"],
            min_count=best_params["min_count"],
            workers=4,
            sg=1,
            epochs=best_params["epochs"],
            alpha=best_params["alpha"],
            seed=42
        )
        
        # ìƒˆ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        new_similarity = calculate_avg_similarity(final_model, test_words)
        logger.info(f"ğŸ“Š ìƒˆ ëª¨ë¸ ì„±ëŠ¥ (í‰ê·  ìœ ì‚¬ë„): {new_similarity}")
        
        # ìƒˆ ëª¨ë¸ ì„ì‹œ ì €ì¥
        final_model.save(new_model_path)
        
        # ì„±ëŠ¥ ë¹„êµ ë° ëª¨ë¸ êµì²´ ê²°ì •
        performance_improved = new_similarity > baseline_similarity
        vocab_size = len(final_model.wv)
        
        training_time = round(time.time() - start_time, 2)
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        today = datetime.now(local_tz)
        stats_record = {
            "date": today.strftime("%Y-%m-%d"),
            "time": today.strftime("%H:%M:%S"),
            "baseline_similarity": baseline_similarity,
            "new_similarity": new_similarity,
            "performance_improved": performance_improved,
            "vocab_size": vocab_size,
            "vector_size": final_model.vector_size,
            "training_sentences": len(all_sentences),
            "new_sentences": len(new_sentences),
            "training_time_sec": training_time,
            "best_params": best_params
        }
        
        # í•™ìŠµ ê¸°ë¡ CSV ì €ì¥
        stats_file = "/opt/airflow/data/model_training_stats.csv"
        file_exists = os.path.exists(stats_file)
        
        with open(stats_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=stats_record.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(stats_record)
        
        # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
        context['task_instance'].xcom_push(key='new_model_path', value=new_model_path)
        context['task_instance'].xcom_push(key='performance_improved', value=performance_improved)
        context['task_instance'].xcom_push(key='baseline_similarity', value=baseline_similarity)
        context['task_instance'].xcom_push(key='new_similarity', value=new_similarity)
        context['task_instance'].xcom_push(key='vocab_size', value=vocab_size)
        
        logger.info(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {training_time}ì´ˆ ì†Œìš”")
        
        return {
            "training_success": True,
            "performance_improved": performance_improved,
            "baseline_similarity": baseline_similarity,
            "new_similarity": new_similarity,
            "vocab_size": vocab_size,
            "training_time": training_time,
            "new_model_path": new_model_path
        }
        
    except ImportError as e:
        logger.error(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
        return {"training_success": False, "reason": f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}"}
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
        return {"training_success": False, "reason": str(e)}

def deploy_new_model(**context):
    """ì„±ëŠ¥ì´ í–¥ìƒëœ ê²½ìš° ìƒˆ ëª¨ë¸ì„ ë°°í¬"""
    logger.info("=== ëª¨ë¸ ë°°í¬ ê²°ì • ===")
    
    # ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    performance_improved = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='performance_improved'
    )
    new_model_path = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='new_model_path'
    )
    baseline_similarity = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='baseline_similarity'
    )
    new_similarity = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='new_similarity'
    )
    
    if not new_model_path or not os.path.exists(new_model_path):
        logger.error("âŒ ìƒˆ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"deployed": False, "reason": "ëª¨ë¸ íŒŒì¼ ì—†ìŒ"}
    
    current_model_path = "/opt/airflow/models/word2vec_custom.model"
    backup_model_path = "/opt/airflow/models/word2vec_custom_backup.model"
    
    try:
        if performance_improved:
            logger.info(f"ğŸš€ ì„±ëŠ¥ í–¥ìƒ í™•ì¸: {baseline_similarity} â†’ {new_similarity}")
            
            # ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ í›„ ìƒˆ ëª¨ë¸ ë°°í¬
            if os.path.exists(current_model_path):
                if os.path.isdir(current_model_path):
                    shutil.rmtree(current_model_path)
                else:
                    os.remove(current_model_path)
            
            # ìƒˆ ëª¨ë¸ì„ ë©”ì¸ ê²½ë¡œë¡œ ì´ë™
            shutil.move(new_model_path, current_model_path)
            
            logger.info("âœ… ìƒˆ ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
            
            # AI ì„œë¹„ìŠ¤ì— ëª¨ë¸ ë¦¬ë¡œë“œ ìš”ì²­
            reload_success = request_ai_service_reload()
            
            return {
                "deployed": True,
                "performance_improved": True,
                "baseline_similarity": baseline_similarity,
                "new_similarity": new_similarity,
                "ai_reload_success": reload_success
            }
        
        else:
            logger.info(f"ğŸ“‰ ì„±ëŠ¥ ì €í•˜ ë˜ëŠ” ë³€í™” ì—†ìŒ: {baseline_similarity} vs {new_similarity}")
            logger.info("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ìœ ì§€")
            
            # ìƒˆ ëª¨ë¸ íŒŒì¼ ì‚­ì œ
            if os.path.exists(new_model_path):
                if os.path.isdir(new_model_path):
                    shutil.rmtree(new_model_path)
                else:
                    os.remove(new_model_path)
            
            return {
                "deployed": False,
                "performance_improved": False,
                "baseline_similarity": baseline_similarity,
                "new_similarity": new_similarity,
                "reason": "ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ"
            }
    
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨: {e}")
        return {"deployed": False, "reason": str(e)}

def request_ai_service_reload():
    """AI ì„œë¹„ìŠ¤ì— ëª¨ë¸ ë¦¬ë¡œë“œ ìš”ì²­"""
    try:
        reload_url = f"{AI_SERVICE_URL}/reload-model"
        response = requests.post(reload_url, timeout=60)
        
        if response.status_code == 200:
            logger.info("âœ… AI ì„œë¹„ìŠ¤ ëª¨ë¸ ë¦¬ë¡œë“œ ì„±ê³µ")
            return True
        else:
            logger.warning(f"âš ï¸ AI ì„œë¹„ìŠ¤ ë¦¬ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
            return False
    
    except Exception as e:
        logger.error(f"âŒ AI ì„œë¹„ìŠ¤ ë¦¬ë¡œë“œ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return False

def trigger_failed_games_retry(**context):
    """ëª¨ë¸ ì—…ë°ì´íŠ¸ í›„ ì‹¤íŒ¨í•œ ê²Œì„ë“¤ ì¬ì‹œë„ ì„¤ì •"""
    logger.info("=== ì‹¤íŒ¨ ê²Œì„ ì¬ì‹œë„ ì„¤ì • ===")
    
    # ëª¨ë¸ì´ ë°°í¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    deployed = context['task_instance'].xcom_pull(
        task_ids='deploy_model',
        key='deployed'
    )
    
    if not deployed:
        logger.info("â„¹ï¸ ëª¨ë¸ì´ ë°°í¬ë˜ì§€ ì•Šì•„ ì¬ì‹œë„ ìŠ¤í‚µ")
        return {"retry_set": False, "reason": "ëª¨ë¸ ë°°í¬ ì•ˆë¨"}
    
    try:
        with db_manager.get_connection() as conn:
            cursor = conn.cursor(buffered=True)
            
            # "ëª¨ë¸ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" ì˜¤ë¥˜ë¡œ ì‹¤íŒ¨í•œ ê²Œì„ë“¤ì„ ë‹¤ì‹œ ëŒ€ê¸° ìƒíƒœë¡œ ë³€ê²½
            query = """
            UPDATE game_detail 
            SET ai_status_code = 'B20005',
                description = 'ëª¨ë¸ ì—…ë°ì´íŠ¸ í›„ ì¬ì‹œë„ ëŒ€ê¸°',
                ai_processed_at = NULL
            WHERE ai_status_code = 'B20008'
            AND description LIKE '%ëª¨ë¸ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤%'
            AND ai_processed_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)
            """
            
            cursor.execute(query)
            conn.commit()
            
            retry_count = cursor.rowcount
            
            # ì•ˆì „í•œ ì»¤ì„œ ë‹«ê¸°
            try:
                while cursor.nextset():
                    pass
            except:
                pass
            cursor.close()
            
            logger.info(f"âœ… {retry_count}ê°œ ì‹¤íŒ¨ ê²Œì„ì„ ì¬ì‹œë„ ëŒ€ê¸°ë¡œ ì„¤ì •")
            
            return {
                "retry_set": True,
                "retry_count": retry_count
            }
    
    except Exception as e:
        logger.error(f"âŒ ì¬ì‹œë„ ì„¤ì • ì‹¤íŒ¨: {e}")
        return {"retry_set": False, "reason": str(e)}

# DAG ì •ì˜
word_trainer_default_args = {
    **DAG_DEFAULT_ARGS,
    'start_date': datetime(2024, 1, 1, tzinfo=local_tz),
    'retries': 1,  # í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì¬ì‹œë„ ìµœì†Œí™”
    'retry_delay': timedelta(minutes=10),
}

word_trainer_dag = DAG(
    'memory_forest_word_trainer',
    default_args=word_trainer_default_args,
    description='Memory Forest Word2Vec ëª¨ë¸ ì¬í•™ìŠµ ë° ë°°í¬',
    schedule_interval='0 3 * * *',  # ë§¤ì¼ ìƒˆë²½ 3ì‹œ ì‹¤í–‰ (ìˆ˜ì§‘ ì™„ë£Œ í›„)
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'word2vec', 'model-training', 'ai-model']
)

# Task ì •ì˜
start_training = DummyOperator(
    task_id='start_training',
    dag=word_trainer_dag
)

check_data_task = PythonOperator(
    task_id='check_training_data',
    python_callable=check_training_data_available,
    dag=word_trainer_dag
)

train_model_task = PythonOperator(
    task_id='train_model',
    python_callable=train_enhanced_word2vec,
    dag=word_trainer_dag,
    pool='cpu_intensive',  # CPU ì§‘ì•½ì  ì‘ì—…ì„ ìœ„í•œ í’€ (Airflow ì„¤ì • í•„ìš”)
)

deploy_model_task = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_new_model,
    dag=word_trainer_dag
)

retry_games_task = PythonOperator(
    task_id='retry_failed_games',
    python_callable=trigger_failed_games_retry,
    dag=word_trainer_dag
)

# ë©”ì¸ DAG íŠ¸ë¦¬ê±° (ì‹¤íŒ¨ ê²Œì„ ì¬ì²˜ë¦¬ë¥¼ ìœ„í•´)
trigger_main_dag = TriggerDagRunOperator(
    task_id='trigger_main_processing',
    trigger_dag_id='memory_forest_ai_main',
    dag=word_trainer_dag,
    wait_for_completion=False
)

end_training = DummyOperator(
    task_id='end_training',
    dag=word_trainer_dag
)

# Task ì˜ì¡´ì„±
start_training >> check_data_task >> train_model_task >> deploy_model_task >> retry_games_task >> trigger_main_dag >> end_training