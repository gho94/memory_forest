"""
Memory Forest Word2Vec ëª¨ë¸ ì¬í•™ìŠµ DAG - ê¸°ì¡´ ì½”ë“œ í˜¸í™˜
ìˆ˜ì§‘ëœ í•™ìŠµ ë°ì´í„°ë¡œ Word2Vec ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ê³  ì„±ëŠ¥ ë¹„êµ í›„ êµì²´
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta
import pendulum
import logging
import json
import os
import shutil
import time
import csv
import requests
from typing import Dict, List

# ë¡œì»¬ ëª¨ë“ˆ import
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import (
    DAG_DEFAULT_ARGS, AI_SERVICE_CONFIG, SCHEDULES, DEFAULT_ARGS, LOCAL_TZ,
    MODEL_TRAINING_CONFIG, DATA_PATHS
)
from utils.database import db_manager
from utils.ai_service import ai_client

logger = logging.getLogger(__name__)

# AI ì„œë¹„ìŠ¤ URL
AI_SERVICE_URL = AI_SERVICE_CONFIG['base_url']

def check_training_data_available(**context):
    """í•™ìŠµ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    logger.info("=== í•™ìŠµ ë°ì´í„° ê°€ìš©ì„± í™•ì¸ ===")
    
    today = datetime.now(LOCAL_TZ).strftime("%Y%m%d")
    training_data_file = f"{DATA_PATHS['model_training']}/{today}/processed_sentences.json"
    
    if not os.path.exists(training_data_file):
        logger.warning(f"âš ï¸ í•™ìŠµ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {training_data_file}")
        return {"training_ready": False, "reason": "í•™ìŠµ ë°ì´í„° íŒŒì¼ ì—†ìŒ"}
    
    try:
        with open(training_data_file, 'r', encoding='utf-8') as f:
            sentences = json.load(f)
        
        min_sentences = MODEL_TRAINING_CONFIG['min_sentences_required']
        if len(sentences) < min_sentences:
            logger.warning(f"âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(sentences)}ê°œ ë¬¸ì¥ (ìµœì†Œ {min_sentences}ê°œ í•„ìš”)")
            return {"training_ready": False, "reason": f"ë°ì´í„° ë¶€ì¡± ({len(sentences)}ê°œ)"}
        
        logger.info(f"âœ… í•™ìŠµ ë°ì´í„° í™•ì¸ ì™„ë£Œ: {len(sentences)}ê°œ ë¬¸ì¥")
        
        # XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
        context['task_instance'].xcom_push(
            key='training_data_file', 
            value=training_data_file
        )
        context['task_instance'].xcom_push(
            key='sentences_count', 
            value=len(sentences)
        )
        
        return {
            "training_ready": True,
            "sentences_count": len(sentences),
            "data_file": training_data_file
        }
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
        return {"training_ready": False, "reason": f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"}

def train_enhanced_word2vec(**context):
    """ê¸°ì¡´ ëª¨ë¸ + ìƒˆ ë°ì´í„°ë¡œ Word2Vec ëª¨ë¸ ì¬í•™ìŠµ"""
    logger.info("=== Word2Vec ëª¨ë¸ ì¬í•™ìŠµ ì‹œì‘ ===")
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ í•™ìŠµ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    training_data_file = context['task_instance'].xcom_pull(
        task_ids='check_training_data',
        key='training_data_file'
    )
    sentences_count = context['task_instance'].xcom_pull(
        task_ids='check_training_data',
        key='sentences_count'
    )
    
    if not training_data_file:
        logger.error("âŒ í•™ìŠµ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"training_success": False, "reason": "í•™ìŠµ ë°ì´í„° ì—†ìŒ"}
    
    try:
        # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
        import numpy as np
        import optuna
        from gensim.models import Word2Vec
        from numpy import dot
        from numpy.linalg import norm
        
        start_time = time.time()
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (DATA_PATHS ì‚¬ìš©)
        models_dir = DATA_PATHS['models']
        current_model_path = f"{models_dir}/word2vec_custom.model"
        backup_model_path = f"{models_dir}/word2vec_custom_backup.model"
        new_model_path = f"{models_dir}/word2vec_custom_new.model"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(models_dir, exist_ok=True)
        
        # ê¸°ì¡´ ëª¨ë¸ ë°±ì—…
        if os.path.exists(current_model_path):
            if os.path.exists(backup_model_path):
                if os.path.isdir(backup_model_path):
                    shutil.rmtree(backup_model_path)
                else:
                    os.remove(backup_model_path)
            shutil.copy2(current_model_path, backup_model_path)
            logger.info("ğŸ“¦ ê¸°ì¡´ ëª¨ë¸ ë°±ì—… ì™„ë£Œ")
        
        # ìƒˆ í•™ìŠµ ë°ì´í„° ë¡œë“œ
        with open(training_data_file, 'r', encoding='utf-8') as f:
            new_sentences = json.load(f)
        
        logger.info(f"ğŸ“š ìƒˆ í•™ìŠµ ë°ì´í„° ë¡œë“œ: {len(new_sentences)}ê°œ ë¬¸ì¥")
        
        # ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ê²°í•©
        all_sentences = new_sentences.copy()
        
        if os.path.exists(current_model_path):
            try:
                existing_model = Word2Vec.load(current_model_path)
                logger.info(f"ğŸ“– ê¸°ì¡´ ëª¨ë¸ ì–´íœ˜ í¬ê¸°: {len(existing_model.wv)}")
                
                # ê¸°ì¡´ ì–´íœ˜ë¥¼ í™œìš©í•œ ê°€ìƒ ë¬¸ì¥ ìƒì„±
                existing_vocab = list(existing_model.wv.key_to_index.keys())
                
                # ê¸°ì¡´ ì–´íœ˜ë¥¼ í¬í•¨í•œ ë¬¸ì¥ë“¤ì„ ì¼ë¶€ ì¶”ê°€ (ë‹¤ì–‘ì„± í™•ë³´)
                synthetic_count = min(100, len(existing_vocab) // 10)
                for i in range(synthetic_count):
                    start_idx = i * 10
                    end_idx = min(start_idx + 10, len(existing_vocab))
                    if end_idx > start_idx:
                        synthetic_sentence = existing_vocab[start_idx:end_idx]
                        all_sentences.append(synthetic_sentence)
                
                logger.info(f"ğŸ“ˆ ê¸°ì¡´ ì–´íœ˜ í†µí•© ì™„ë£Œ: ì´ {len(all_sentences)}ê°œ ë¬¸ì¥")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ í•™ìŠµ: {e}")
        
        # ì„±ëŠ¥ í‰ê°€ìš© ê¸°ì¤€ ë‹¨ì–´ë“¤ (ê¸°ì¡´ AI ì„œë¹„ìŠ¤ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ë“¤)
        test_words = [
            # ê°€ì¡± ê´€ë ¨
            "ë¶€ëª¨", "ì•„ë²„ì§€", "ì–´ë¨¸ë‹ˆ", "ì•„ë“¤", "ë”¸", "í˜•ì œ", "ìë§¤", "í• ë¨¸ë‹ˆ", "í• ì•„ë²„ì§€",
            # ê°ì • ê´€ë ¨  
            "ê¸°ì¨", "ìŠ¬í””", "ì‚¬ë‘", "í–‰ë³µ", "ê±±ì •", "ë‘ë ¤ì›€", "í¬ë§", "ê°ë™", "ê·¸ë¦¬ì›€",
            # ì¼ìƒ ê´€ë ¨
            "ìŒì‹", "ì§‘", "í•™êµ", "ë³‘ì›", "íšŒì‚¬", "ì¹œêµ¬", "ì„ ìƒë‹˜", "ì˜ì‚¬", "ê°„í˜¸ì‚¬",
            # ë™ë¬¼ ê´€ë ¨
            "ê°•ì•„ì§€", "ê³ ì–‘ì´", "ìƒˆ", "ë¬¼ê³ ê¸°", "í† ë¼", "í–„ìŠ¤í„°", "ê±°ë¶ì´", "ì•µë¬´ìƒˆ",
            # ê³„ì ˆ/ìì—° ê´€ë ¨
            "ë´„", "ì—¬ë¦„", "ê°€ì„", "ê²¨ìš¸", "ë°”ë‹¤", "ì‚°", "ê°•", "í•˜ëŠ˜", "êµ¬ë¦„", "ë³„"
        ]
        
        def calculate_avg_similarity(model, words):
            """í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° - ê¸°ì¡´ AI ì„œë¹„ìŠ¤ ë¡œì§ê³¼ ìœ ì‚¬"""
            vectors = []
            for word in words:
                if word in model.wv:
                    vectors.append(model.wv[word])
            
            if len(vectors) < 2:
                return 0.0
            
            similarities = []
            for i in range(len(vectors)):
                for j in range(i + 1, len(vectors)):
                    # ê¸°ì¡´ AI ì„œë¹„ìŠ¤ì™€ ê°™ì€ ê³„ì‚° ë°©ì‹
                    vec1, vec2 = vectors[i], vectors[j]
                    norm1, norm2 = norm(vec1), norm(vec2)
                    if norm1 > 0 and norm2 > 0:
                        sim = dot(vec1, vec2) / (norm1 * norm2)
                        similarities.append(sim)
            
            return round(np.mean(similarities), 4) if similarities else 0.0
        
        # ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        baseline_similarity = 0.0
        if os.path.exists(current_model_path):
            try:
                baseline_model = Word2Vec.load(current_model_path)
                baseline_similarity = calculate_avg_similarity(baseline_model, test_words)
                logger.info(f"ğŸ“Š ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ (í‰ê·  ìœ ì‚¬ë„): {baseline_similarity}")
            except:
                logger.warning("âš ï¸ ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
        
        # Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (configì—ì„œ ì„¤ì •ê°’ ì‚¬ìš©)
        def objective(trial):
            vector_size = trial.suggest_categorical("vector_size", MODEL_TRAINING_CONFIG['vector_size_options'])
            window = trial.suggest_int("window", *MODEL_TRAINING_CONFIG['window_range'])
            min_count = trial.suggest_int("min_count", *MODEL_TRAINING_CONFIG['min_count_range'])
            epochs = trial.suggest_int("epochs", *MODEL_TRAINING_CONFIG['epochs_range'])
            alpha = trial.suggest_float("alpha", *MODEL_TRAINING_CONFIG['alpha_range'])
            
            try:
                model = Word2Vec(
                    sentences=all_sentences,
                    vector_size=vector_size,
                    window=window,
                    min_count=min_count,
                    workers=4,
                    sg=1,  # Skip-gram (ê¸°ì¡´ AI ì„œë¹„ìŠ¤ì™€ ë™ì¼)
                    epochs=epochs,
                    alpha=alpha,
                    seed=42
                )
                
                score = calculate_avg_similarity(model, test_words)
                return score
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œë„ ì‹¤íŒ¨: {e}")
                return 0.0
        
        # ìµœì í™” ì‹¤í–‰ (configì—ì„œ ì„¤ì •ê°’ ì‚¬ìš©)
        logger.info("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        study = optuna.create_study(direction="maximize")
        study.optimize(
            objective, 
            n_trials=MODEL_TRAINING_CONFIG['optimization_trials'], 
            timeout=MODEL_TRAINING_CONFIG['optimization_timeout']
        )
        
        best_params = study.best_params
        logger.info(f"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        logger.info("ğŸš€ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        final_model = Word2Vec(
            sentences=all_sentences,
            vector_size=best_params["vector_size"],
            window=best_params["window"],
            min_count=best_params["min_count"],
            workers=4,
            sg=1,  # ê¸°ì¡´ AI ì„œë¹„ìŠ¤ì™€ ë™ì¼í•œ Skip-gram
            epochs=best_params["epochs"],
            alpha=best_params["alpha"],
            seed=42
        )
        
        # ìƒˆ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
        new_similarity = calculate_avg_similarity(final_model, test_words)
        logger.info(f"ğŸ“Š ìƒˆ ëª¨ë¸ ì„±ëŠ¥ (í‰ê·  ìœ ì‚¬ë„): {new_similarity}")
        
        # ìƒˆ ëª¨ë¸ ì„ì‹œ ì €ì¥
        final_model.save(new_model_path)
        
        # ì„±ëŠ¥ ë¹„êµ ë° ëª¨ë¸ êµì²´ ê²°ì •
        performance_improved = new_similarity > baseline_similarity
        vocab_size = len(final_model.wv)
        
        training_time = round(time.time() - start_time, 2)
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        today = datetime.now(LOCAL_TZ)
        stats_record = {
            "date": today.strftime("%Y-%m-%d"),
            "time": today.strftime("%H:%M:%S"),
            "baseline_similarity": baseline_similarity,
            "new_similarity": new_similarity,
            "performance_improved": performance_improved,
            "vocab_size": vocab_size,
            "vector_size": final_model.vector_size,
            "training_sentences": len(all_sentences),
            "new_sentences": len(new_sentences),
            "training_time_sec": training_time,
            "best_params": best_params
        }
        
        # í•™ìŠµ ê¸°ë¡ CSV ì €ì¥
        stats_file = f"{DATA_PATHS['data']}/model_training_stats.csv"
        os.makedirs(os.path.dirname(stats_file), exist_ok=True)
        file_exists = os.path.exists(stats_file)
        
        with open(stats_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=stats_record.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(stats_record)
        
        # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
        context['task_instance'].xcom_push(key='new_model_path', value=new_model_path)
        context['task_instance'].xcom_push(key='performance_improved', value=performance_improved)
        context['task_instance'].xcom_push(key='baseline_similarity', value=baseline_similarity)
        context['task_instance'].xcom_push(key='new_similarity', value=new_similarity)
        context['task_instance'].xcom_push(key='vocab_size', value=vocab_size)
        
        logger.info(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {training_time}ì´ˆ ì†Œìš”")
        
        return {
            "training_success": True,
            "performance_improved": performance_improved,
            "baseline_similarity": baseline_similarity,
            "new_similarity": new_similarity,
            "vocab_size": vocab_size,
            "training_time": training_time,
            "new_model_path": new_model_path
        }
        
    except ImportError as e:
        logger.error(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
        return {"training_success": False, "reason": f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}"}
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
        return {"training_success": False, "reason": str(e)}

def deploy_new_model(**context):
    """ì„±ëŠ¥ì´ í–¥ìƒëœ ê²½ìš° ìƒˆ ëª¨ë¸ì„ ë°°í¬"""
    logger.info("=== ëª¨ë¸ ë°°í¬ ê²°ì • ===")
    
    # ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    performance_improved = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='performance_improved'
    )
    new_model_path = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='new_model_path'
    )
    baseline_similarity = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='baseline_similarity'
    )
    new_similarity = context['task_instance'].xcom_pull(
        task_ids='train_model',
        key='new_similarity'
    )
    
    if not new_model_path or not os.path.exists(new_model_path):
        logger.error("âŒ ìƒˆ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"deployed": False, "reason": "ëª¨ë¸ íŒŒì¼ ì—†ìŒ"}
    
    models_dir = DATA_PATHS['models']
    current_model_path = f"{models_dir}/word2vec_custom.model"
    
    try:
        if performance_improved:
            logger.info(f"ğŸš€ ì„±ëŠ¥ í–¥ìƒ í™•ì¸: {baseline_similarity} â†’ {new_similarity}")
            
            # ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ í›„ ìƒˆ ëª¨ë¸ ë°°í¬
            if os.path.exists(current_model_path):
                if os.path.isdir(current_model_path):
                    shutil.rmtree(current_model_path)
                else:
                    os.remove(current_model_path)
            
            # ìƒˆ ëª¨ë¸ì„ ë©”ì¸ ê²½ë¡œë¡œ ì´ë™
            shutil.move(new_model_path, current_model_path)
            
            logger.info("âœ… ìƒˆ ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
            
            # AI ì„œë¹„ìŠ¤ì— ëª¨ë¸ ë¦¬ë¡œë“œ ìš”ì²­ (ê¸°ì¡´ ai_client ì‚¬ìš©)
            reload_success = ai_client.reload_model()
            
            # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
            context['task_instance'].xcom_push(key='deployed', value=True)
            
            return {
                "deployed": True,
                "performance_improved": True,
                "baseline_similarity": baseline_similarity,
                "new_similarity": new_similarity,
                "ai_reload_success": reload_success
            }
        
        else:
            logger.info(f"ğŸ“‰ ì„±ëŠ¥ ì €í•˜ ë˜ëŠ” ë³€í™” ì—†ìŒ: {baseline_similarity} vs {new_similarity}")
            logger.info("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ìœ ì§€")
            
            # ìƒˆ ëª¨ë¸ íŒŒì¼ ì‚­ì œ
            if os.path.exists(new_model_path):
                if os.path.isdir(new_model_path):
                    shutil.rmtree(new_model_path)
                else:
                    os.remove(new_model_path)
            
            # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
            context['task_instance'].xcom_push(key='deployed', value=False)
            
            return {
                "deployed": False,
                "performance_improved": False,
                "baseline_similarity": baseline_similarity,
                "new_similarity": new_similarity,
                "reason": "ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ"
            }
    
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨: {e}")
        context['task_instance'].xcom_push(key='deployed', value=False)
        return {"deployed": False, "reason": str(e)}

def trigger_failed_games_retry(**context):
    """ëª¨ë¸ ì—…ë°ì´íŠ¸ í›„ ì‹¤íŒ¨í•œ ê²Œì„ë“¤ ì¬ì‹œë„ ì„¤ì •"""
    logger.info("=== ì‹¤íŒ¨ ê²Œì„ ì¬ì‹œë„ ì„¤ì • ===")
    
    # ëª¨ë¸ì´ ë°°í¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    deployed = context['task_instance'].xcom_pull(
        task_ids='deploy_model',
        key='deployed'
    )
    
    if not deployed:
        logger.info("â„¹ï¸ ëª¨ë¸ì´ ë°°í¬ë˜ì§€ ì•Šì•„ ì¬ì‹œë„ ìŠ¤í‚µ")
        return {"retry_set": False, "reason": "ëª¨ë¸ ë°°í¬ ì•ˆë¨"}
    
    try:
        # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì‚¬ìš©
        retry_count = db_manager.mark_games_for_retry(
            error_keywords=['ëª¨ë¸ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'],
            max_count=100
        )
        
        logger.info(f"âœ… {retry_count}ê°œ ì‹¤íŒ¨ ê²Œì„ì„ ì¬ì‹œë„ ëŒ€ê¸°ë¡œ ì„¤ì •")
        
        return {
            "retry_set": True,
            "retry_count": retry_count
        }
    
    except Exception as e:
        logger.error(f"âŒ ì¬ì‹œë„ ì„¤ì • ì‹¤íŒ¨: {e}")
        return {"retry_set": False, "reason": str(e)}

# DAG ì •ì˜
word_trainer_default_args = {
    **DEFAULT_ARGS,
    'start_date': datetime(2024, 1, 1, tzinfo=LOCAL_TZ),
    'retries': 1,  # í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì¬ì‹œë„ ìµœì†Œí™”
    'retry_delay': timedelta(minutes=10),
}

word_trainer_dag = DAG(
    'memory_forest_word_trainer',
    default_args=word_trainer_default_args,
    description='Memory Forest Word2Vec ëª¨ë¸ ì¬í•™ìŠµ ë° ë°°í¬',
    schedule_interval=SCHEDULES['word_training'],  # ë§¤ì¼ ìƒˆë²½ 3ì‹œ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['memory-forest', 'word2vec', 'model-training', 'ai-model']
)

# Task ì •ì˜
start_training = DummyOperator(
    task_id='start_training',
    dag=word_trainer_dag
)

check_data_task = PythonOperator(
    task_id='check_training_data',
    python_callable=check_training_data_available,
    dag=word_trainer_dag
)

train_model_task = PythonOperator(
    task_id='train_model',
    python_callable=train_enhanced_word2vec,
    dag=word_trainer_dag
)

deploy_model_task = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_new_model,
    dag=word_trainer_dag
)

retry_games_task = PythonOperator(
    task_id='retry_failed_games',
    python_callable=trigger_failed_games_retry,
    dag=word_trainer_dag
)

# ë©”ì¸ DAG íŠ¸ë¦¬ê±° (ì‹¤íŒ¨ ê²Œì„ ì¬ì²˜ë¦¬ë¥¼ ìœ„í•´)
trigger_main_dag = TriggerDagRunOperator(
    task_id='trigger_main_processing',
    trigger_dag_id='memory_forest_ai_main',
    dag=word_trainer_dag,
    wait_for_completion=False
)

end_training = DummyOperator(
    task_id='end_training',
    dag=word_trainer_dag
)

# Task ì˜ì¡´ì„±
start_training >> check_data_task >> train_model_task >> deploy_model_task >> retry_games_task >> trigger_main_dag >> end_training